{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'locatedIn', 1: 'neighborOf'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from pudb import set_trace;\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_curve as prc\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "with open('data/countries_s3') as f:\n",
    "    facts = f.read().splitlines()\n",
    "facts = [el.split(',') for el in facts]\n",
    "preds = [fact[0] for fact in facts]\n",
    "subjs = [fact[1] for fact in facts]\n",
    "objs = [fact[2] for fact in facts]\n",
    "\n",
    "unique = sorted(list(set(preds)))\n",
    "num_unique = len(unique)\n",
    "num_predicates = num_unique\n",
    "predsToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToPreds = dict(zip(range(num_unique),unique))\n",
    "print(idxToPreds)\n",
    "\n",
    "unique = sorted(list(set(subjs+objs)))\n",
    "num_unique = len(unique)\n",
    "num_constants = num_unique\n",
    "consToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToCons = dict(zip(range(num_unique),unique))\n",
    "\n",
    "facts = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(facts))])\n",
    "data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "\n",
    "#data: idx0->predicate, idx1->subj, idx2->obj\n",
    "data[facts[:,0],facts[:,1],facts[:,2]] = 1\n",
    "no_facts = int(np.sum(data))\n",
    "data = torch.from_numpy(data)\n",
    "\n",
    "predicates = torch.eye(num_predicates)\n",
    "constants = torch.eye(num_constants)\n",
    "\n",
    "knowledge_pos = (data==1).nonzero()\n",
    "data_aux = knowledge_pos\n",
    "knowledge_pos = torch.cat((predicates[knowledge_pos[:,0]],\n",
    "                           constants[knowledge_pos[:,1]],\n",
    "                           constants[knowledge_pos[:,2]]),dim=1)\n",
    "\n",
    "num_feats_per_fact = knowledge_pos.size()[1]\n",
    "\n",
    "# reading the test set\n",
    "with open('data/s1_test') as f:\n",
    "    test = f.read().splitlines()\n",
    "test = [el.split(',') for el in test]\n",
    "preds = [fact[0] for fact in test]\n",
    "subjs = [fact[1] for fact in test]\n",
    "objs = [fact[2] for fact in test]\n",
    "\n",
    "test = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(test))])\n",
    "ts_data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "ts_data[test[:,0],test[:,1],test[:,2]] = 1\n",
    "ts_data = torch.from_numpy(ts_data)\n",
    "test = (ts_data==1).nonzero()\n",
    "test = torch.cat((predicates[test[:,0]],\n",
    "                   constants[test[:,1]],\n",
    "                   constants[test[:,2]]),dim=1)\n",
    "\n",
    "#reading the complete database (to sample false facts)\n",
    "with open('data/countries_complete') as f:\n",
    "    comp = f.read().splitlines()\n",
    "comp = [el.split(',') for el in comp]\n",
    "preds = [fact[0] for fact in comp]\n",
    "subjs = [fact[1] for fact in comp]\n",
    "objs = [fact[2] for fact in comp]\n",
    "\n",
    "comp = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(comp))])\n",
    "ts_data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "ts_data[comp[:,0],comp[:,1],comp[:,2]] = 1\n",
    "ts_data = torch.from_numpy(ts_data)\n",
    "comp = (ts_data==1).nonzero()\n",
    "comp = torch.cat((predicates[comp[:,0]],\n",
    "                   constants[comp[:,1]],\n",
    "                   constants[comp[:,2]]),dim=1)\n",
    "\n",
    "#takes a fact in embedding form and prints its word equivalent\n",
    "def print_fact(fact):\n",
    "    p = int(fact[:num_predicates].nonzero().squeeze().numpy())\n",
    "    p = idxToPreds[p]\n",
    "    c1 = int(fact[num_predicates:num_predicates+num_constants].nonzero().squeeze().numpy())\n",
    "    c1 = idxToCons[c1]\n",
    "    c2 = int(fact[num_predicates+num_constants:num_predicates+2*num_constants].nonzero().squeeze().numpy())\n",
    "    c2 = idxToCons[c2]\n",
    "    print(p,c1,c2)\n",
    "\n",
    "#gets a set of false facts for each test fact (s[i,j] by sampling c1, c2, c3 and c4 such that s[c1,j], s[i,c2] and s[c3,c4]\n",
    "# do not belong to the complete dataset\n",
    "def get_false_set(test,comp):\n",
    "    test_tmp = test.numpy()\n",
    "    comp_tmp = comp.numpy()\n",
    "    false_set = torch.empty(0,num_feats_per_fact)\n",
    "    for el in test_tmp:\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            f1 = np.concatenate((el[:num_predicates],c1,el[num_predicates+num_constants:]))\n",
    "            if not any((comp_tmp[:]==f1).all(1)):\n",
    "                f1 = torch.from_numpy(f1).unsqueeze(0)\n",
    "                break\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            f2 = np.concatenate((el[:num_predicates+num_constants],c1))\n",
    "            if not any((comp_tmp[:]==f2).all(1)):\n",
    "                f2 = torch.from_numpy(f2).unsqueeze(0)\n",
    "                break\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            c2 = constants[np.random.randint(0,num_constants),:]\n",
    "            f3 = np.concatenate((el[:num_predicates],c1,c2))\n",
    "            if not any((comp_tmp[:]==f3).all(1)):\n",
    "                f3 = torch.from_numpy(f3).unsqueeze(0)\n",
    "                break\n",
    "        false_set = torch.cat((false_set,f1,f2,f3),dim=0)\n",
    "    return false_set\n",
    "\n",
    "\n",
    "# sample num_samples as a connected subgraph of the input data\n",
    "#   basically, perform num_samples steps, each adding one more fact\n",
    "#   that is connected to at least one constant in the sample\n",
    "# data: a tensor of the form num_preds,num_cons,num_cons where a 1\n",
    "#   means that the fact composed of pred,cons1,cons2 is true\n",
    "# num_samples: number of samples to be gotten\n",
    "# returns sample: a tensor of the form num_samples*num_feats_per_fact\n",
    "def sample_neighbors(num_samples,data):\n",
    "    data_source_tmp = data.clone()\n",
    "    data_tmp = torch.zeros_like(data)\n",
    "    sample = torch.zeros(0,num_feats_per_fact,dtype=torch.long)\n",
    "    #choose one random constant\n",
    "    idx = torch.randperm(num_constants)[0].unsqueeze(0)\n",
    "    for _ in range(num_samples):\n",
    "    #     print('data_source',data_source_tmp)\n",
    "        #subset your possible choices to where idx is subject or object\n",
    "        data_tmp[:,idx,:] = data_source_tmp[:,idx,:]\n",
    "        data_tmp[:,:,idx] = data_source_tmp[:,:,idx]\n",
    "        #choose one at random\n",
    "        new_fact = data_tmp.nonzero()\n",
    "        if new_fact.size()[0] == 0:\n",
    "            break\n",
    "        chosen = torch.randperm(new_fact.size()[0])[0]\n",
    "        new_fact = new_fact[chosen,:].unsqueeze(0)\n",
    "        #add fact to sample\n",
    "        sample = torch.cat((sample,new_fact),dim=0)\n",
    "        #set chosen fact to zero (avoiding choosing it again)\n",
    "        data_source_tmp[new_fact[:,0],new_fact[:,1],new_fact[:,2]] = 0\n",
    "        #add new idx in the fact\n",
    "        idx = torch.cat((idx,new_fact[:,1],new_fact[:,2]))\n",
    "        idx = torch.unique(idx)\n",
    "    sample = torch.cat((predicates[sample[:,0]],\n",
    "                        constants[sample[:,1]],\n",
    "                        constants[sample[:,2]]),dim=1)\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "#helps removing repeated predicted facts -> same embeddings and constants, probably different scores\n",
    "#this is of complexity K^3, could be optimized\n",
    "def leaveTopK(preds,K):\n",
    "    _,idx = torch.sort(preds[:,-4],descending=True)\n",
    "    preds = preds[idx,:]\n",
    "    out = preds[0,:].unsqueeze(0)\n",
    "    for i in range(1,min(K,preds.size()[0])):\n",
    "        t = preds[i,:].unsqueeze(0)\n",
    "        if t[:,-4] == 0:\n",
    "            break\n",
    "        m,_ = torch.max(F.cosine_similarity(t[:,:-4].repeat(out.size()[0],1),out[:,:-4],dim=1),dim=0)\n",
    "        if m<1:\n",
    "            out = torch.cat((out,t),dim=0)    \n",
    "    return out\n",
    "\n",
    "####FORWARD CHAINING\n",
    "#input: some facts that can either be ground or predicted in previous steps\n",
    "#output: predicted facts in this specific step (outer loop must gather all of them)\n",
    "#computes the predictions of applying each rule to the facts, giving a score to each of them.\n",
    "#leaves only topK scoring fact for each applied rule (not for the whole thing)\n",
    "def forward_step(facts,K):\n",
    "    num_facts = facts.size()[0]\n",
    "    facts = torch.cat((facts,torch.range(0,facts.size()[0]-1).unsqueeze(1)),dim=1)\n",
    "    facts_tmp = facts.clone()\n",
    "    facts = facts.repeat((1,num_predicates)).view(-1,num_feats_per_fact+2)\n",
    "    #rule 1\n",
    "    # b1(x,y)<-b2(y,x)\n",
    "    preds_expanded = predicates.repeat((num_facts,1))\n",
    "    start_time = time.time()\n",
    "    rule_expanded = rules[0].repeat(facts.size()[0],1)\n",
    "#     print(rule_expanded)\n",
    "#     print(facts)\n",
    "#     print(preds_expanded)\n",
    "    #body unification\n",
    "    preds_r1 = F.cosine_similarity(rule_expanded[:,:num_predicates],facts[:,:num_predicates],dim=1)\n",
    "#     print('body', preds_r1)\n",
    "    #previous score\n",
    "    preds_r1 = preds_r1*facts[:,-2]\n",
    "#     print('previous', preds_r1)\n",
    "    #head unification (for each predicate)\n",
    "    preds_r1 = preds_r1 * F.cosine_similarity(preds_expanded,rule_expanded[:,:num_predicates])\n",
    "#     print('head', preds_r1)\n",
    "#     print(preds_r1)\n",
    "    preds_r1 = preds_r1.unsqueeze(1)\n",
    "    preds_r1 = torch.cat((preds_expanded,\n",
    "                         facts[:,num_predicates+num_constants:-2],\n",
    "                         facts[:,num_predicates:num_predicates+num_constants],\n",
    "                         preds_r1,\n",
    "                         facts[:,-1].unsqueeze(1),\n",
    "                         -torch.ones(facts.size()[0],1),\n",
    "                         -torch.ones(facts.size()[0],1)),dim=1)\n",
    "    # print(preds_r1)\n",
    "    preds_r1 = leaveTopK(preds_r1,K)\n",
    "\n",
    "\n",
    "    \n",
    "    # #rule 2\n",
    "    # #b1(x,y)<-b2(x,z),b3(z,y)\n",
    "    # #plus 2 because the fact_id is now the last dimmension\n",
    "    # body1 = facts.repeat((1,num_facts)).view(-1,num_feats_per_fact+2)\n",
    "    # body2 = facts.repeat((num_facts,1))\n",
    "    # rule_expanded = rules[1].repeat(body1.size()[0],1)\n",
    "    # preds_expanded = predicates.repeat(num_facts**2,1)\n",
    "    # #previous scores\n",
    "    # preds_r2 = body1[:,-2]*body2[:,-2]\n",
    "    # #similarity between shared constants\n",
    "    # preds_r2 = preds_r2*F.cosine_similarity(body1[:,num_predicates+num_constants:-2],\n",
    "    #                                         body2[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    # #remove 0 scores to improve speed - if constant isn't shared we don't compute anything else\n",
    "\n",
    "    # non_zero = preds_r2.nonzero().squeeze()\n",
    "    # if non_zero.size()[0]==0:\n",
    "    #     preds_r2 = torch.zeros_like(preds_r2)\n",
    "    # else:\n",
    "    #     #predicate of body1 with predicate of rule\\\n",
    "    #     preds_r2[non_zero] = preds_r2[non_zero]*F.cosine_similarity(rule_expanded[non_zero,num_predicates:2*num_predicates],\n",
    "    #                                                                 body1[non_zero,:num_predicates],dim=1)\n",
    "\n",
    "    # if non_zero.size()[0]==0:\n",
    "    #     preds_r2 = torch.zeros_like(preds_r2)\n",
    "    # else:\n",
    "    #     #predicate of body2 with predicate of rule\n",
    "    #     preds_r2[non_zero] = preds_r2[non_zero]*F.cosine_similarity(rule_expanded[non_zero,num_predicates:2*num_predicates],\n",
    "    #                                                                 body2[non_zero,:num_predicates],dim=1)\n",
    "    # if non_zero.size()[0]==0:\n",
    "    #     preds_r2 = torch.zeros_like(preds_r2)\n",
    "    # else:\n",
    "    #     #head of rule with the two predicates\n",
    "    #     preds_r2[non_zero] = preds_r2[non_zero]*F.cosine_similarity(rule_expanded[non_zero,:num_predicates],\n",
    "    #                                                                 preds_expanded[non_zero,:],dim=1)\n",
    "    \n",
    "    # preds_r2 = preds_r2.unsqueeze(1)\n",
    "    # preds_r2 = torch.cat((preds_expanded\n",
    "    #                      ,body1[:,num_predicates:num_predicates+num_constants]\n",
    "    #                      ,body2[:,num_predicates+num_constants:-2]\n",
    "    #                      ,preds_r2\n",
    "    #                      ,body1[:,-1].unsqueeze(1)\n",
    "    #                      ,body2[:,-1].unsqueeze(1))\n",
    "    #                     ,dim=1)\n",
    "    # #removing repeated facts and leaving ones with highest score\n",
    "    # preds_r2 = leaveTopK(preds_r2,K)\n",
    "\n",
    "    #rule 3\n",
    "    #b1(x,y)<-b2(x,z),b3(z,w),b4(w,y)\n",
    "    #plus 2 because the fact_id is now the last dimmension\n",
    "    body1 = facts.repeat((1,num_facts)).view(-1,num_feats_per_fact+2)\n",
    "    body2 = facts.repeat((num_facts,1))\n",
    "    rule_expanded = rules[1].repeat(body1.size()[0],1)\n",
    "    preds_expanded = predicates.repeat(num_facts**2,1)\n",
    "    #previous scores\n",
    "    preds_r3 = body1[:,-2]*body2[:,-2]\n",
    "    #similarity between shared constants\n",
    "    preds_r3 = preds_r3*F.cosine_similarity(body1[:,num_predicates+num_constants:-2],\n",
    "                                            body2[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    #remove 0 scores to improve speed - if constant isn't shared we don't compute anything else\n",
    "\n",
    "    non_zero = preds_r3.nonzero().squeeze()\n",
    "    if non_zero.size()[0]==0:\n",
    "        preds_r3 = torch.zeros_like(preds_r3)\n",
    "    else:\n",
    "        #predicate of body1 with predicate of rule\\\n",
    "        preds_r3[non_zero] = preds_r3[non_zero]*F.cosine_similarity(rule_expanded[non_zero,num_predicates:2*num_predicates],\n",
    "                                                                    body1[non_zero,:num_predicates],dim=1)\n",
    "        #predicate of body2 with predicate of rule\n",
    "        preds_r3[non_zero] = preds_r3[non_zero]*F.cosine_similarity(rule_expanded[non_zero,2*num_predicates:3*num_predicates],\n",
    "                                                                    body2[non_zero,:num_predicates],dim=1)\n",
    "        #head of rule with the two predicates\n",
    "        preds_r3[non_zero] = preds_r3[non_zero]*F.cosine_similarity(rule_expanded[non_zero,:num_predicates],\n",
    "                                                                    preds_expanded[non_zero,:],dim=1)\n",
    "    \n",
    "    preds_r3 = preds_r3.unsqueeze(1)\n",
    "    preds_r3 = torch.cat((preds_expanded\n",
    "                         ,body1[:,num_predicates:num_predicates+num_constants]\n",
    "                         ,body2[:,num_predicates+num_constants:-2]\n",
    "                         ,preds_r3\n",
    "                         ,body1[:,-1].unsqueeze(1)\n",
    "                         ,body2[:,-1].unsqueeze(1),\n",
    "                         -torch.ones(body1.size()[0],1))\n",
    "                        ,dim=1)\n",
    "    \n",
    "    #removing repeated facts and leaving ones with highest score\n",
    "    preds_r3 = leaveTopK(preds_r3,K)\n",
    "\n",
    "#     #taking care of third atom\n",
    "    no_preds_left = preds_r3.size()[0]\n",
    "    body3 = facts_tmp.repeat((1,no_preds_left)).view(-1,num_feats_per_fact+2)\n",
    "\n",
    "    preds_r4 = preds_r3.repeat((num_facts,1))\n",
    "\n",
    "    rule_expanded = rules[1].repeat((preds_r4.size()[0],1))\n",
    "    \n",
    "    #unifying second shared constant\n",
    "    p = preds_r4[:,-4] * F.cosine_similarity(preds_r4[:,num_predicates+num_constants:num_predicates+2*num_constants],\n",
    "                                            body3[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    p = p * F.cosine_similarity(preds_r4[:,num_predicates+num_constants:num_predicates+2*num_constants],\n",
    "                                            body3[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    \n",
    "    #unifying third body predicate\n",
    "    p = p * F.cosine_similarity(rule_expanded[:,3*num_predicates:],\n",
    "                                            body3[:,:num_predicates],dim=1)\n",
    "#     for i in (preds_r4[:,:num_predicates+num_constants]\n",
    "#                          ,body3[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "#                          ,p\n",
    "#                          ,preds_r4[:,[-3,-2]]\n",
    "#                          ,body3[:,-1].unsqueeze(1)):\n",
    "#               print(i.size())\n",
    "    preds_r5 = torch.cat((preds_r4[:,:num_predicates+num_constants]\n",
    "                         ,body3[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "                         ,p.unsqueeze(1)\n",
    "                         ,preds_r4[:,[-3,-2]]\n",
    "                         ,body3[:,-1].unsqueeze(1))\n",
    "                        ,dim=1)\n",
    "    \n",
    "    #removing repeated facts and leaving ones with highest score\n",
    "    preds_r5 = leaveTopK(preds_r5,K)\n",
    "    #out = torch.cat((preds_r1,preds_r2,preds_r3),dim=0)\n",
    "    out = torch.cat((preds_r1,preds_r5),dim=0)\n",
    "#     print(\"fws took %s\" % (time.time() - start_time))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.2235,  0.7765]), tensor([ 0.2478,  0.7519,  0.2277,  0.7705,  0.2287,  0.7622,  0.2478,\n",
      "         0.7519])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2327,  0.7635]), tensor([ 0.2373,  0.7420,  0.2230,  0.7769,  0.2262,  0.7691,  0.2272,\n",
      "         0.7665])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2265,  0.7666]), tensor([ 0.2341,  0.7592,  0.2249,  0.7721,  0.2245,  0.7753,  0.2343,\n",
      "         0.7574])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2200,  0.7775]), tensor([ 0.2234,  0.7764,  0.2155,  0.7738,  0.2139,  0.7850,  0.2256,\n",
      "         0.7712])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2256,  0.7665]), tensor([ 0.2291,  0.7580,  0.2176,  0.7807,  0.2177,  0.7804,  0.2212,\n",
      "         0.7786])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2017,  0.7737]), tensor([ 0.2132,  0.7762,  0.2072,  0.7815,  0.2050,  0.7842,  0.2149,\n",
      "         0.7701])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2057,  0.7826]), tensor([ 0.2329,  0.7671,  0.2231,  0.7662,  0.2224,  0.7760,  0.2335,\n",
      "         0.7644])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2133,  0.7783]), tensor([ 0.2018,  0.7933,  0.1939,  0.8023,  0.1937,  0.7998,  0.2016,\n",
      "         0.7918])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2320,  0.7619]), tensor([ 0.2356,  0.7573,  0.2267,  0.7693,  0.2266,  0.7693,  0.2341,\n",
      "         0.7596])]\n",
      "auc 0.625\n",
      "[tensor([ 0.2145,  0.7840]), tensor([ 0.2038,  0.7922,  0.2007,  0.7993,  0.2005,  0.7995,  0.2015,\n",
      "         0.7984])]\n",
      "auc 0.625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Find maximum similarity for each consequence in the set of facts contained in target\n",
    "#if testing is true it finds the consequence with maximum similarity for each target\n",
    "#otherwise finds the target with maximum similarity for each consequence\n",
    "#Inputs: \n",
    "#consequences: result of unrolling the rules for the specified steps with the input facts\n",
    "#target: set of facts that are assumed to be true\n",
    "#testing: returns the probabilities of matched facts, a prediction is considered true if p>0.5\n",
    "def find_max_similarities(consequences,target,testing=False):\n",
    "    start_time = time.time()    \n",
    "    num_consequences = consequences.size()[0]\n",
    "    num_targets = target.size()[0]\n",
    "\n",
    "    #each consequence repeated by the number of targets\n",
    "    if testing:\n",
    "        #for each target find max similarity across consequences\n",
    "        tmp_c = consequences.repeat(num_targets,1)\n",
    "        tmp_t = target.repeat(1,num_consequences).view(-1,num_feats_per_fact)\n",
    "    else:\n",
    "        #for each consequence compute the similarity with all targets\n",
    "        tmp_c = consequences.repeat(1,num_targets).view(-1,num_feats_per_fact+4)\n",
    "        tmp_t = target.repeat(num_consequences,1)\n",
    "\n",
    "    #first constant\n",
    "    sim = F.cosine_similarity(tmp_c[:,num_predicates:num_predicates+num_constants],\n",
    "                              tmp_t[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    #only compute for non-zero values to speed up\n",
    "#     non_zero = sim.nonzero()\n",
    "#     if non_zero.size()[0]==0:\n",
    "#         sim = torch.zeros_like(sim)\n",
    "#     else:\n",
    "#         non_zero = non_zero.squeeze()\n",
    "#         sim[non_zero] = sim[non_zero] * F.cosine_similarity(tmp_c[non_zero,num_predicates+num_constants:-3]\n",
    "#                                                            ,tmp_t[non_zero,num_predicates+num_constants:],dim=1)\n",
    "\n",
    "#     non_zero = sim.nonzero()\n",
    "#     if non_zero.size()[0]==0:\n",
    "#         sim = torch.zeros_like(sim)\n",
    "#     else:\n",
    "#         non_zero = non_zero.squeeze()\n",
    "#         sim[non_zero] = sim[non_zero] * F.cosine_similarity(tmp_c[non_zero,:num_predicates] \n",
    "#                                                            ,tmp_t[non_zero,:num_predicates],dim=1)\n",
    "#         sim[non_zero] = sim[non_zero] + tmp_c[non_zero,-3]*lamb2\n",
    "    sim = sim * F.cosine_similarity(tmp_c[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "                                    ,tmp_t[:,num_predicates+num_constants:],dim=1)\n",
    "    sim = sim * F.cosine_similarity(tmp_c[:,:num_predicates] \n",
    "                                    ,tmp_t[:,:num_predicates],dim=1)\n",
    "    sim = sim + tmp_c[:,-3]*lamb2\n",
    "    #for each consequence/target, get the maximum simlarity with the set of targets/consequences\n",
    "    if testing:\n",
    "        sim = sim.view(-1,num_consequences)\n",
    "    else:\n",
    "        sim = sim.view(-1,num_targets)\n",
    "    m, idx = torch.max(sim,dim=1)\n",
    "#     print(\"fms took %s\" % (time.time() - start_time))\n",
    "    if testing:\n",
    "        return m, consequences[idx,:]\n",
    "    return m,target[idx,:]\n",
    "\n",
    "\n",
    "\n",
    "####TRAINING\n",
    "#dbg -> cherrypicked\n",
    "# core_rel = Variable(knowledge_pos[[0,1,3]])\n",
    "# target = Variable(knowledge_pos[2]).unsqueeze(0)\n",
    "# target = Variable(knowledge_pos[[2,4],:])\n",
    "#####sampling\n",
    "target = Variable(knowledge_pos)\n",
    "no_samples = 40\n",
    "\n",
    "num_iters = 50\n",
    "learning_rate = .1\n",
    "lamb = 1\n",
    "lamb2 = 0\n",
    "\n",
    "steps = 1\n",
    "num_rules = 3\n",
    "epsilon=.001\n",
    "\n",
    "K = 400 ##For top K\n",
    "\n",
    "#hyperparameter search\n",
    "# lambdas = [1,2,5,0.3,0.8]\n",
    "with open('s3_auc-pr','w') as f:\n",
    "    # for lamb in lambdas:\n",
    "    suc_rate_neigh = 0\n",
    "    suc_rate_locin = 0\n",
    "    accuracies = []\n",
    "    for _ in range(10):\n",
    "        K_tmp = K\n",
    "        #rules should be:\n",
    "        #r1(x,y) <- r2(y,x)\n",
    "        #r1(x,y) <- r2(x,z),r3(z,x)\n",
    "        rules = [Variable(torch.rand(1*num_predicates), requires_grad=True),\n",
    "                 # Variable(torch.rand(2*num_predicates), requires_grad=True),\n",
    "                 Variable(torch.rand(4*num_predicates), requires_grad=True)]\n",
    "        # rules = [Variable(torch.rand(num_predicates), requires_grad=True),\n",
    "        #          Variable(torch.Tensor([1, 1]), requires_grad=True)]\n",
    "        f.write('random_rules' + str(rules) + '\\n')\n",
    "        optimizer = torch.optim.Adam([\n",
    "                {'params': rules}], \n",
    "                lr = learning_rate)\n",
    "\n",
    "        criterion = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "        rules_tmp = [torch.zeros_like(rule) for rule in rules]\n",
    "        for epoch in range(num_iters):\n",
    "            for par in optimizer.param_groups:\n",
    "#                 par['params'][2].data.clamp_(min=0.3,max=0.7)\n",
    "                par['params'][1].data.clamp_(min=0.3,max=0.7)\n",
    "                par['params'][0].data.clamp_(min=0.3,max=0.7)\n",
    "\n",
    "            # core_rel = Variable(knowledge_pos[core_rel])\n",
    "            core_rel = sample_neighbors(no_samples,data)\n",
    "            # target = Variable(knowledge_pos)\n",
    "            optimizer.zero_grad()\n",
    "            facts = torch.cat((core_rel, Variable(torch.ones(core_rel.size()[0], 1))), 1)\n",
    "            #will accumulate predictions separately to compare with target facts\n",
    "            consequences = forward_step(facts,K_tmp)\n",
    "            for step in range(1,steps):\n",
    "                tmp = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "                tmp = forward_step(tmp,K_tmp)\n",
    "                consequences = torch.cat((consequences,tmp),dim=0)\n",
    "            #LOSS\n",
    "            loss = 0\n",
    "            m, matches = find_max_similarities(consequences,core_rel,testing=True)\n",
    "            loss = torch.sum(m*(1 - matches[:,-4]))\n",
    "#             print(epoch, 'losssssssssssssssssssss',loss.data[0])\n",
    "            # print(sum([torch.sum(rules_tmp[i]-rules[i]) for i in range(num_rules)]))\n",
    "#             if loss < 10**-6 or sum([torch.sum(torch.abs(rules_tmp[i]-rules[i])) for i in range(num_rules)])<10**-5:\n",
    "#                 break\n",
    "            rules_tmp = [r.clone() for r in rules]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(rules)\n",
    "        ###### printing and saving AUC\n",
    "        K_tmp = 500\n",
    "        facts = torch.cat((knowledge_pos, Variable(torch.ones(knowledge_pos.size()[0], 1))), 1)\n",
    "        consequences = forward_step(facts,K_tmp)\n",
    "        for step in range(1,steps):\n",
    "            tmp = torch.cat((facts,consequences[:,:-3]),dim=0)\n",
    "            tmp = forward_step(tmp,K_tmp)\n",
    "            consequences = torch.cat((consequences,tmp),dim=0)\n",
    "        false_set = get_false_set(test,comp)\n",
    "        test_plus_false = torch.cat((test,false_set))\n",
    "        m,matches = find_max_similarities(consequences,test_plus_false,testing=True)\n",
    "        p = matches[:,-4]\n",
    "        true_vals = np.ones(test.size()[0])\n",
    "        true_vals = np.concatenate((true_vals,np.zeros(false_set.size()[0])))\n",
    "        prec,rec,_ = prc(true_vals,(m*p).detach().numpy()) #assume that no unification is a score of 0\n",
    "        auc_tmp = auc(rec,prec)\n",
    "        print('auc',auc_tmp)\n",
    "        f.write('rules' + str(rules) + '\\n')\n",
    "        f.write('auc' + str(auc_tmp) + '\\n')\n",
    "#######Writting results\n",
    "    #     suc_neigh, suc_locIn = False,False\n",
    "    #     if F.cosine_similarity(rules[0],torch.Tensor([0,1,0,1]),dim=0)>0.5:\n",
    "    #         suc_neigh = True\n",
    "    #     if F.cosine_similarity(rules[1],torch.Tensor([1,0,1,0,1,0]),dim=0)>0.5:\n",
    "    #         suc_locIn = True\n",
    "    #     if suc_neigh:\n",
    "    #         suc_rate_neigh+=1\n",
    "    #     if suc_locIn:\n",
    "    #         suc_rate_locin+=1\n",
    "    #     f.write('lamb '+str(lamb)+'\\n')\n",
    "    #     f.write('train_loss '+str(loss)+'\\n')\n",
    "    #     f.write('rules '+str(rules)+'\\n')\n",
    "    #     f.write('suc_neigh '+str(suc_neigh)+'\\n')\n",
    "    #     f.write('suc_locIn '+str(suc_locIn)+'\\n')\n",
    "    #     #computing test results\n",
    "    #     print('computing test results')\n",
    "    #     K_tmp = 250\n",
    "    #     facts = torch.cat((knowledge_pos, Variable(torch.ones(knowledge_pos.size()[0], 1))), 1)\n",
    "    #     consequences = forward_step(facts,K_tmp)\n",
    "    #     for step in range(1,steps):\n",
    "    #         tmp = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "    #         tmp = forward_step(tmp,K_tmp)\n",
    "    #         consequences = torch.cat((consequences,tmp),dim=0)\n",
    "    #     m,matches = find_max_similarities(consequences,test,testing=True)\n",
    "    #     p = matches[:,-3]\n",
    "    #     true_positives = m[p>0.5]\n",
    "    #     true_positives = (true_positives>0.5).nonzero()\n",
    "    #     true_positives = true_positives.size()[0]\n",
    "    #     ts_accuracy = true_positives/test.size()[0]\n",
    "    #     accuracies.append(ts_accuracy)\n",
    "    #     f.write('ts_accuracy '+str(ts_accuracy)+'\\n')\n",
    "    #     f.flush()\n",
    "    # f.write('#############RESULTS###############'+'\\n')\n",
    "    # f.write('lamb '+str(lamb)+'\\n')\n",
    "    # f.write('suc_rate_neigh '+str(suc_rate_neigh)+'\\n')\n",
    "    # f.write('suc_rate_locin '+str(suc_rate_locin)+'\\n')\n",
    "    # accuracies = np.array(accuracies)\n",
    "    # f.write('mean accuracy ' + str(np.mean(accuracies)) +'\\n')\n",
    "    # f.write('std accuracy ' + str(np.std(accuracies))+'\\n')\n",
    "    # f.write('####################################'+'\\n')\n",
    "    # f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3280, 548])"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TRAINING\n",
    "#dbg -> cherrypicked\n",
    "# core_rel = Variable(knowledge_pos[[0,1,3]])\n",
    "# target = Variable(knowledge_pos[2]).unsqueeze(0)\n",
    "# target = Variable(knowledge_pos[[2,4],:])\n",
    "#####sampling\n",
    "no_samples = 40\n",
    "\n",
    "learning_rate = .1\n",
    "lamb = 1\n",
    "lamb2 = 0\n",
    "\n",
    "steps = 1\n",
    "num_rules = 2\n",
    "epsilon=.001\n",
    "rules = [Variable(torch.rand(1*num_predicates), requires_grad=True),\n",
    "                 # Variable(torch.rand(2*num_predicates), requires_grad=True),\n",
    "                 Variable(torch.rand(4*num_predicates), requires_grad=True)]\n",
    "K = 400 ##For top K\n",
    "K_tmp = K\n",
    "core_rel = sample_neighbors(no_samples,data)\n",
    "facts = torch.cat((core_rel, Variable(torch.ones(core_rel.size()[0], 1))), 1)\n",
    "consequences = forward_step(facts,K_tmp)\n",
    "consequences.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.8007,  0.1992,  0.8005,  0.1993]), tensor([ 0.7259,  0.2739,  0.4354,  0.6134,  0.7232,  0.2740])]\n",
      "tensor([[ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 0.,  1.]])\n",
      "mmmmmmmm tensor([ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([[  1.0000,   0.0000,   0.0000,  ...,   0.9417,  17.0000,\n",
      "          -1.0000],\n",
      "        [  0.0000,   1.0000,   0.0000,  ...,   0.0583,  11.0000,\n",
      "          -1.0000],\n",
      "        [  1.0000,   0.0000,   0.0000,  ...,   0.9417,  17.0000,\n",
      "          -1.0000],\n",
      "        ...,\n",
      "        [  1.0000,   0.0000,   0.0000,  ...,   0.9417,  17.0000,\n",
      "          -1.0000],\n",
      "        [  1.0000,   0.0000,   0.0000,  ...,   0.9417,  17.0000,\n",
      "          -1.0000],\n",
      "        [  1.0000,   0.0000,   0.0000,  ...,   0.9417,  17.0000,\n",
      "          -1.0000]])\n",
      "tensor([ 0.0000,  0.9417,  0.0000,  0.0000,  0.0000,  0.8980,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.9417,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n",
      "tensor([[[ 0.,  1.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 0.,  1.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.]]])\n",
      "tensor([[ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 11.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [  9.,   3.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [  1.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.],\n",
      "        [ 17.,  -1.]])\n",
      "tensor([ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "neighborOf bulgaria macedonia\n",
      "neighborOf bulgaria turkey\n",
      "locatedIn macedonia europe\n",
      "neighborOf albania macedonia\n",
      "locatedIn ukraine europe\n",
      "neighborOf kosovo macedonia\n",
      "neighborOf greece bulgaria\n",
      "locatedIn belgium europe\n",
      "locatedIn greece europe\n",
      "neighborOf kosovo albania\n",
      "neighborOf georgia turkey\n",
      "neighborOf turkey bulgaria\n",
      "neighborOf kosovo serbia\n",
      "locatedIn sweden europe\n",
      "neighborOf finland sweden\n",
      "neighborOf iraq turkey\n",
      "locatedIn serbia southern_europe\n",
      "locatedIn moldova europe\n",
      "locatedIn slovenia europe\n",
      "neighborOf armenia turkey\n",
      "-----------------\n",
      "neighborOf bulgaria macedonia\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf bulgaria turkey\n",
      "neighborOf bulgaria turkey\n",
      "neighborOf turkey bulgaria\n",
      "-----------------\n",
      "locatedIn macedonia europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf albania macedonia\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn ukraine europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf kosovo macedonia\n",
      "neighborOf kosovo macedonia\n",
      "neighborOf kosovo albania\n",
      "neighborOf albania macedonia\n",
      "-----------------\n",
      "neighborOf greece bulgaria\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn belgium europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn greece europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf kosovo albania\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf georgia turkey\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf turkey bulgaria\n",
      "neighborOf turkey bulgaria\n",
      "neighborOf bulgaria turkey\n",
      "-----------------\n",
      "neighborOf kosovo serbia\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn sweden europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf finland sweden\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf iraq turkey\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn serbia southern_europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn moldova europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "locatedIn slovenia europe\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n",
      "-----------------\n",
      "neighborOf armenia turkey\n",
      "locatedIn europe moldova\n",
      "locatedIn moldova europe\n"
     ]
    }
   ],
   "source": [
    "# rules = [torch.Tensor([ 0.3,  0.7,  0.3,  0.7]),\n",
    "#          torch.Tensor([ 0.3,  0.7,0.3,  0.7,0.3,  0.7,])]\n",
    "lamb2=0\n",
    "print(rules)\n",
    "# facts = sample_neighbors(20,data)\n",
    "# target = facts\n",
    "print(target[:,:num_predicates])\n",
    "# facts = torch.cat((facts, Variable(torch.ones(facts.size()[0], 1))), 1)\n",
    "consequences = forward_step(facts,150)\n",
    "# print(consequences)\n",
    "for step in range(1,steps):\n",
    "    tmp = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "    tmp = forward_step(tmp,150)\n",
    "    consequences = torch.cat((consequences,tmp),dim=0)\n",
    "m,matches = find_max_similarities(consequences,target,True)\n",
    "print('mmmmmmmm',m)\n",
    "print(matches)\n",
    "print(m*(1-matches[:,-3]))\n",
    "#writting an interpreter\n",
    "allFacts = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "print(target[torch.round(1-m).nonzero(),:num_predicates])\n",
    "print(matches[:,[-2,-1]])\n",
    "print(m)\n",
    "for i in range(target.size()[0]):\n",
    "    print_fact(target[i,:])\n",
    "for i in range(target.size()[0]):\n",
    "    print('-----------------')\n",
    "    print_fact(target[i,:])\n",
    "    print_fact(matches[i,:])\n",
    "    print_fact(facts[matches[i,-2].type(torch.LongTensor),:])\n",
    "    if matches[i,-1] != -1:\n",
    "        print_fact(facts[matches[i,-1].type(torch.LongTensor),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locatedIn hungary eastern_europe\n",
      "neighborOf croatia hungary\n",
      "neighborOf hungary croatia\n",
      "locatedIn serbia southern_europe\n",
      "locatedIn afghanistan asia\n"
     ]
    }
   ],
   "source": [
    "# def get_path(idx,matches):\n",
    "#     print(idx,'pred',matches[idx,:num_predicates],matches[idx,num_predicates:num_predicates+].non_)\n",
    "#     if match[[-2,-1]]==[-1,-1]:\n",
    "#         return\n",
    "#     if -1 not in match[[-2,-1]]:\n",
    "#         get_path\n",
    "def print_fact(fact):\n",
    "    p = int(fact[:num_predicates].nonzero().squeeze().numpy())\n",
    "    p = idxToPreds[p]\n",
    "    c1 = int(fact[num_predicates:num_predicates+num_constants].nonzero().squeeze().numpy())\n",
    "    c1 = idxToCons[c1]\n",
    "    c2 = int(fact[num_predicates+num_constants:num_predicates+2*num_constants].nonzero().squeeze().numpy())\n",
    "    c2 = idxToCons[c2]\n",
    "    print(p,c1,c2)\n",
    "print_fact(facts[17])\n",
    "print_fact(facts[9])\n",
    "print_fact(facts[3])\n",
    "print_fact(facts[1])\n",
    "print_fact(target[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'locatedIn', 1: 'neighborOf'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from pudb import set_trace;\n",
    "import time\n",
    "\n",
    "with open('data/countries_s2') as f:\n",
    "    facts = f.read().splitlines()\n",
    "facts = [el.split(',') for el in facts]\n",
    "preds = [fact[0] for fact in facts]\n",
    "subjs = [fact[1] for fact in facts]\n",
    "objs = [fact[2] for fact in facts]\n",
    "\n",
    "unique = sorted(list(set(preds)))\n",
    "num_unique = len(unique)\n",
    "num_predicates = num_unique\n",
    "predsToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToPreds = dict(zip(range(num_unique),unique))\n",
    "print(idxToPreds)\n",
    "\n",
    "unique = sorted(list(set(subjs+objs)))\n",
    "num_unique = len(unique)\n",
    "num_constants = num_unique\n",
    "consToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToCons = dict(zip(range(num_unique),unique))\n",
    "\n",
    "facts = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(facts))])\n",
    "data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "\n",
    "#data: idx0->predicate, idx1->subj, idx2->obj\n",
    "data[facts[:,0],facts[:,1],facts[:,2]] = 1\n",
    "no_facts = int(np.sum(data))\n",
    "data = torch.from_numpy(data)\n",
    "\n",
    "predicates = torch.eye(num_predicates)\n",
    "constants = torch.eye(num_constants)\n",
    "\n",
    "knowledge_pos = (data==1).nonzero()\n",
    "data_aux = knowledge_pos\n",
    "knowledge_pos = torch.cat((predicates[knowledge_pos[:,0]],\n",
    "                           constants[knowledge_pos[:,1]],\n",
    "                           constants[knowledge_pos[:,2]]),dim=1)\n",
    "\n",
    "num_feats_per_fact = knowledge_pos.size()[1]\n",
    "\n",
    "# reading the test set\n",
    "with open('data/s1_test') as f:\n",
    "    test = f.read().splitlines()\n",
    "test = [el.split(',') for el in test]\n",
    "preds = [fact[0] for fact in test]\n",
    "subjs = [fact[1] for fact in test]\n",
    "objs = [fact[2] for fact in test]\n",
    "\n",
    "test = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(test))])\n",
    "ts_data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "ts_data[test[:,0],test[:,1],test[:,2]] = 1\n",
    "ts_data = torch.from_numpy(ts_data)\n",
    "test = (ts_data==1).nonzero()\n",
    "test = torch.cat((predicates[test[:,0]],\n",
    "                   constants[test[:,1]],\n",
    "                   constants[test[:,2]]),dim=1)\n",
    "\n",
    "#reading the complete database (to sample false facts)\n",
    "with open('data/countries_complete') as f:\n",
    "    comp = f.read().splitlines()\n",
    "comp = [el.split(',') for el in comp]\n",
    "preds = [fact[0] for fact in comp]\n",
    "subjs = [fact[1] for fact in comp]\n",
    "objs = [fact[2] for fact in comp]\n",
    "\n",
    "comp = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(comp))])\n",
    "ts_data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "ts_data[comp[:,0],comp[:,1],comp[:,2]] = 1\n",
    "ts_data = torch.from_numpy(ts_data)\n",
    "comp = (ts_data==1).nonzero()\n",
    "comp = torch.cat((predicates[comp[:,0]],\n",
    "                   constants[comp[:,1]],\n",
    "                   constants[comp[:,2]]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fact(fact):\n",
    "    p = int(fact[:num_predicates].nonzero().squeeze().numpy())\n",
    "    p = idxToPreds[p]\n",
    "    c1 = int(fact[num_predicates:num_predicates+num_constants].nonzero().squeeze().numpy())\n",
    "    c1 = idxToCons[c1]\n",
    "    c2 = int(fact[num_predicates+num_constants:num_predicates+2*num_constants].nonzero().squeeze().numpy())\n",
    "    c2 = idxToCons[c2]\n",
    "    print(p,c1,c2)\n",
    "def get_false_set(test,comp):\n",
    "    test_tmp = test.numpy()\n",
    "    comp_tmp = comp.numpy()\n",
    "    false_set = torch.empty(0,num_feats_per_fact)\n",
    "    for el in test_tmp:\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            f1 = np.concatenate((el[:num_predicates],c1,el[num_predicates+num_constants:]))\n",
    "            if not any((comp_tmp[:]==f1).all(1)):\n",
    "                f1 = torch.from_numpy(f1).unsqueeze(0)\n",
    "                break\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            f2 = np.concatenate((el[:num_predicates+num_constants],c1))\n",
    "            if not any((comp_tmp[:]==f2).all(1)):\n",
    "                f2 = torch.from_numpy(f2).unsqueeze(0)\n",
    "                break\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            c2 = constants[np.random.randint(0,num_constants),:]\n",
    "            f3 = np.concatenate((el[:num_predicates],c1,c2))\n",
    "            if not any((comp_tmp[:]==f3).all(1)):\n",
    "                f3 = torch.from_numpy(f3).unsqueeze(0)\n",
    "                break\n",
    "        false_set = torch.cat((false_set,f1,f2,f3),dim=0)\n",
    "    return false_set\n",
    "\n",
    "false_set = get_false_set(test,comp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-290-e3c0a8b7141f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rules' is not defined"
     ]
    }
   ],
   "source": [
    "print(rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
