{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'locatedIn', 1: 'neighborOf'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from pudb import set_trace;\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_curve as prc\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "with open('data/countries_s3') as f:\n",
    "    facts = f.read().splitlines()\n",
    "facts = [el.split(',') for el in facts]\n",
    "preds = [fact[0] for fact in facts]\n",
    "subjs = [fact[1] for fact in facts]\n",
    "objs = [fact[2] for fact in facts]\n",
    "\n",
    "unique = sorted(list(set(preds)))\n",
    "num_unique = len(unique)\n",
    "num_predicates = num_unique\n",
    "predsToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToPreds = dict(zip(range(num_unique),unique))\n",
    "print(idxToPreds)\n",
    "\n",
    "unique = sorted(list(set(subjs+objs)))\n",
    "num_unique = len(unique)\n",
    "num_constants = num_unique\n",
    "consToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToCons = dict(zip(range(num_unique),unique))\n",
    "\n",
    "facts = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(facts))])\n",
    "data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "\n",
    "#data: idx0->predicate, idx1->subj, idx2->obj\n",
    "data[facts[:,0],facts[:,1],facts[:,2]] = 1\n",
    "no_facts = int(np.sum(data))\n",
    "data = torch.from_numpy(data)\n",
    "\n",
    "predicates = torch.eye(num_predicates)\n",
    "constants = torch.eye(num_constants)\n",
    "\n",
    "knowledge_pos = (data==1).nonzero()\n",
    "data_aux = knowledge_pos\n",
    "knowledge_pos = torch.cat((predicates[knowledge_pos[:,0]],\n",
    "                           constants[knowledge_pos[:,1]],\n",
    "                           constants[knowledge_pos[:,2]]),dim=1)\n",
    "\n",
    "num_feats_per_fact = knowledge_pos.size()[1]\n",
    "\n",
    "# reading the test set\n",
    "with open('data/s1_test') as f:\n",
    "    test = f.read().splitlines()\n",
    "test = [el.split(',') for el in test]\n",
    "preds = [fact[0] for fact in test]\n",
    "subjs = [fact[1] for fact in test]\n",
    "objs = [fact[2] for fact in test]\n",
    "\n",
    "test = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(test))])\n",
    "ts_data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "ts_data[test[:,0],test[:,1],test[:,2]] = 1\n",
    "ts_data = torch.from_numpy(ts_data)\n",
    "test = (ts_data==1).nonzero()\n",
    "test = torch.cat((predicates[test[:,0]],\n",
    "                   constants[test[:,1]],\n",
    "                   constants[test[:,2]]),dim=1)\n",
    "\n",
    "#reading the complete database (to sample false facts)\n",
    "with open('data/countries_complete') as f:\n",
    "    comp = f.read().splitlines()\n",
    "comp = [el.split(',') for el in comp]\n",
    "preds = [fact[0] for fact in comp]\n",
    "subjs = [fact[1] for fact in comp]\n",
    "objs = [fact[2] for fact in comp]\n",
    "\n",
    "comp = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(comp))])\n",
    "ts_data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "ts_data[comp[:,0],comp[:,1],comp[:,2]] = 1\n",
    "ts_data = torch.from_numpy(ts_data)\n",
    "comp = (ts_data==1).nonzero()\n",
    "comp = torch.cat((predicates[comp[:,0]],\n",
    "                   constants[comp[:,1]],\n",
    "                   constants[comp[:,2]]),dim=1)\n",
    "\n",
    "#takes a fact in embedding form and prints its word equivalent\n",
    "def print_fact(fact):\n",
    "    p = int(fact[:num_predicates].nonzero().squeeze().numpy())\n",
    "    p = idxToPreds[p]\n",
    "    c1 = int(fact[num_predicates:num_predicates+num_constants].nonzero().squeeze().numpy())\n",
    "    c1 = idxToCons[c1]\n",
    "    c2 = int(fact[num_predicates+num_constants:num_predicates+2*num_constants].nonzero().squeeze().numpy())\n",
    "    c2 = idxToCons[c2]\n",
    "    print(p,c1,c2)\n",
    "\n",
    "#gets a set of false facts for each test fact (s[i,j] by sampling c1, c2, c3 and c4 such that s[c1,j], s[i,c2] and s[c3,c4]\n",
    "# do not belong to the complete dataset\n",
    "def get_false_set(test,comp):\n",
    "    test_tmp = test.numpy()\n",
    "    comp_tmp = comp.numpy()\n",
    "    false_set = torch.empty(0,num_feats_per_fact)\n",
    "    for el in test_tmp:\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            f1 = np.concatenate((el[:num_predicates],c1,el[num_predicates+num_constants:]))\n",
    "            if not any((comp_tmp[:]==f1).all(1)):\n",
    "                f1 = torch.from_numpy(f1).unsqueeze(0)\n",
    "                break\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            f2 = np.concatenate((el[:num_predicates+num_constants],c1))\n",
    "            if not any((comp_tmp[:]==f2).all(1)):\n",
    "                f2 = torch.from_numpy(f2).unsqueeze(0)\n",
    "                break\n",
    "        while True:\n",
    "            c1 = constants[np.random.randint(0,num_constants),:]\n",
    "            c2 = constants[np.random.randint(0,num_constants),:]\n",
    "            f3 = np.concatenate((el[:num_predicates],c1,c2))\n",
    "            if not any((comp_tmp[:]==f3).all(1)):\n",
    "                f3 = torch.from_numpy(f3).unsqueeze(0)\n",
    "                break\n",
    "        false_set = torch.cat((false_set,f1,f2,f3),dim=0)\n",
    "    return false_set\n",
    "\n",
    "\n",
    "# sample num_samples as a connected subgraph of the input data\n",
    "#   basically, perform num_samples steps, each adding one more fact\n",
    "#   that is connected to at least one constant in the sample\n",
    "# data: a tensor of the form num_preds,num_cons,num_cons where a 1\n",
    "#   means that the fact composed of pred,cons1,cons2 is true\n",
    "# num_samples: number of samples to be gotten\n",
    "# returns sample: a tensor of the form num_samples*num_feats_per_fact\n",
    "def sample_neighbors(num_samples,data):\n",
    "    data_source_tmp = data.clone()\n",
    "    data_tmp = torch.zeros_like(data)\n",
    "    sample = torch.zeros(0,num_feats_per_fact,dtype=torch.long)\n",
    "    #choose one random constant\n",
    "    idx = torch.randperm(num_constants)[0].unsqueeze(0)\n",
    "    for _ in range(num_samples):\n",
    "    #     print('data_source',data_source_tmp)\n",
    "        #subset your possible choices to where idx is subject or object\n",
    "        data_tmp[:,idx,:] = data_source_tmp[:,idx,:]\n",
    "        data_tmp[:,:,idx] = data_source_tmp[:,:,idx]\n",
    "        #choose one at random\n",
    "        new_fact = data_tmp.nonzero()\n",
    "        if new_fact.size()[0] == 0:\n",
    "            break\n",
    "        chosen = torch.randperm(new_fact.size()[0])[0]\n",
    "        new_fact = new_fact[chosen,:].unsqueeze(0)\n",
    "        #add fact to sample\n",
    "        sample = torch.cat((sample,new_fact),dim=0)\n",
    "        #set chosen fact to zero (avoiding choosing it again)\n",
    "        data_source_tmp[new_fact[:,0],new_fact[:,1],new_fact[:,2]] = 0\n",
    "        #add new idx in the fact\n",
    "        idx = torch.cat((idx,new_fact[:,1],new_fact[:,2]))\n",
    "        idx = torch.unique(idx)\n",
    "    sample = torch.cat((predicates[sample[:,0]],\n",
    "                        constants[sample[:,1]],\n",
    "                        constants[sample[:,2]]),dim=1)\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "#helps removing repeated predicted facts -> same embeddings and constants, probably different scores\n",
    "#this is of complexity K^3, could be optimized\n",
    "def leaveTopK(preds,K):\n",
    "    _,idx = torch.sort(preds[:,-4],descending=True)\n",
    "    preds = preds[idx,:]\n",
    "    out = preds[0,:].unsqueeze(0)\n",
    "    for i in range(1,min(K,preds.size()[0])):\n",
    "        t = preds[i,:].unsqueeze(0)\n",
    "        if t[:,-4] == 0:\n",
    "            break\n",
    "        m,_ = torch.max(F.cosine_similarity(t[:,:-4].repeat(out.size()[0],1),out[:,:-4],dim=1),dim=0)\n",
    "        if m<1:\n",
    "            out = torch.cat((out,t),dim=0)    \n",
    "    return out\n",
    "\n",
    "####FORWARD CHAINING\n",
    "#input: some facts that can either be ground or predicted in previous steps\n",
    "#output: predicted facts in this specific step (outer loop must gather all of them)\n",
    "#computes the predictions of applying each rule to the facts, giving a score to each of them.\n",
    "#leaves only topK scoring fact for each applied rule (not for the whole thing)\n",
    "def forward_step(facts,K):\n",
    "    num_facts = facts.size()[0]\n",
    "    facts = torch.cat((facts,torch.range(0,facts.size()[0]-1).unsqueeze(1)),dim=1)\n",
    "    facts_tmp = facts.clone()\n",
    "    facts = facts.repeat((1,num_predicates)).view(-1,num_feats_per_fact+2)\n",
    "    #rule 1\n",
    "    # b1(x,y)<-b2(y,x)\n",
    "    preds_expanded = predicates.repeat((num_facts,1))\n",
    "    start_time = time.time()\n",
    "    rule_expanded = rules[0].repeat(facts.size()[0],1)\n",
    "#     print(rule_expanded)\n",
    "#     print(facts)\n",
    "#     print(preds_expanded)\n",
    "    #body unification\n",
    "    preds_r1 = F.cosine_similarity(rule_expanded[:,:num_predicates],facts[:,:num_predicates],dim=1)\n",
    "#     print('body', preds_r1)\n",
    "    #previous score\n",
    "    preds_r1 = preds_r1*facts[:,-2]\n",
    "#     print('previous', preds_r1)\n",
    "    #head unification (for each predicate)\n",
    "    preds_r1 = preds_r1 * F.cosine_similarity(preds_expanded,rule_expanded[:,:num_predicates])\n",
    "#     print('head', preds_r1)\n",
    "#     print(preds_r1)\n",
    "    preds_r1 = preds_r1.unsqueeze(1)\n",
    "    preds_r1 = torch.cat((preds_expanded,\n",
    "                         facts[:,num_predicates+num_constants:-2],\n",
    "                         facts[:,num_predicates:num_predicates+num_constants],\n",
    "                         preds_r1,\n",
    "                         facts[:,-1].unsqueeze(1),\n",
    "                         -torch.ones(facts.size()[0],1),\n",
    "                         -torch.ones(facts.size()[0],1)),dim=1)\n",
    "    # print(preds_r1)\n",
    "    preds_r1 = leaveTopK(preds_r1,K)\n",
    "\n",
    "\n",
    "    \n",
    "    # #rule 2\n",
    "    # #b1(x,y)<-b2(x,z),b3(z,y)\n",
    "    # #plus 2 because the fact_id is now the last dimmension\n",
    "    # body1 = facts.repeat((1,num_facts)).view(-1,num_feats_per_fact+2)\n",
    "    # body2 = facts.repeat((num_facts,1))\n",
    "    # rule_expanded = rules[1].repeat(body1.size()[0],1)\n",
    "    # preds_expanded = predicates.repeat(num_facts**2,1)\n",
    "    # #previous scores\n",
    "    # preds_r2 = body1[:,-2]*body2[:,-2]\n",
    "    # #similarity between shared constants\n",
    "    # preds_r2 = preds_r2*F.cosine_similarity(body1[:,num_predicates+num_constants:-2],\n",
    "    #                                         body2[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    # #remove 0 scores to improve speed - if constant isn't shared we don't compute anything else\n",
    "\n",
    "    # non_zero = preds_r2.nonzero().squeeze()\n",
    "    # if non_zero.size()[0]==0:\n",
    "    #     preds_r2 = torch.zeros_like(preds_r2)\n",
    "    # else:\n",
    "    #     #predicate of body1 with predicate of rule\\\n",
    "    #     preds_r2[non_zero] = preds_r2[non_zero]*F.cosine_similarity(rule_expanded[non_zero,num_predicates:2*num_predicates],\n",
    "    #                                                                 body1[non_zero,:num_predicates],dim=1)\n",
    "\n",
    "    # if non_zero.size()[0]==0:\n",
    "    #     preds_r2 = torch.zeros_like(preds_r2)\n",
    "    # else:\n",
    "    #     #predicate of body2 with predicate of rule\n",
    "    #     preds_r2[non_zero] = preds_r2[non_zero]*F.cosine_similarity(rule_expanded[non_zero,num_predicates:2*num_predicates],\n",
    "    #                                                                 body2[non_zero,:num_predicates],dim=1)\n",
    "    # if non_zero.size()[0]==0:\n",
    "    #     preds_r2 = torch.zeros_like(preds_r2)\n",
    "    # else:\n",
    "    #     #head of rule with the two predicates\n",
    "    #     preds_r2[non_zero] = preds_r2[non_zero]*F.cosine_similarity(rule_expanded[non_zero,:num_predicates],\n",
    "    #                                                                 preds_expanded[non_zero,:],dim=1)\n",
    "    \n",
    "    # preds_r2 = preds_r2.unsqueeze(1)\n",
    "    # preds_r2 = torch.cat((preds_expanded\n",
    "    #                      ,body1[:,num_predicates:num_predicates+num_constants]\n",
    "    #                      ,body2[:,num_predicates+num_constants:-2]\n",
    "    #                      ,preds_r2\n",
    "    #                      ,body1[:,-1].unsqueeze(1)\n",
    "    #                      ,body2[:,-1].unsqueeze(1))\n",
    "    #                     ,dim=1)\n",
    "    # #removing repeated facts and leaving ones with highest score\n",
    "    # preds_r2 = leaveTopK(preds_r2,K)\n",
    "\n",
    "    #rule 3\n",
    "    #b1(x,y)<-b2(x,z),b3(z,w),b4(w,y)\n",
    "    #plus 2 because the fact_id is now the last dimmension\n",
    "    body1 = facts.repeat((1,num_facts)).view(-1,num_feats_per_fact+2)\n",
    "    body2 = facts.repeat((num_facts,1))\n",
    "    rule_expanded = rules[1].repeat(body1.size()[0],1)\n",
    "    preds_expanded = predicates.repeat(num_facts**2,1)\n",
    "    #previous scores\n",
    "    preds_r3 = body1[:,-2]*body2[:,-2]\n",
    "    #similarity between shared constants\n",
    "    preds_r3 = preds_r3*F.cosine_similarity(body1[:,num_predicates+num_constants:-2],\n",
    "                                            body2[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    #remove 0 scores to improve speed - if constant isn't shared we don't compute anything else\n",
    "\n",
    "    non_zero = preds_r3.nonzero().squeeze()\n",
    "    if non_zero.size()[0]==0:\n",
    "        preds_r3 = torch.zeros_like(preds_r3)\n",
    "    else:\n",
    "        #predicate of body1 with predicate of rule\\\n",
    "        preds_r3[non_zero] = preds_r3[non_zero]*F.cosine_similarity(rule_expanded[non_zero,num_predicates:2*num_predicates],\n",
    "                                                                    body1[non_zero,:num_predicates],dim=1)\n",
    "        #predicate of body2 with predicate of rule\n",
    "        preds_r3[non_zero] = preds_r3[non_zero]*F.cosine_similarity(rule_expanded[non_zero,2*num_predicates:3*num_predicates],\n",
    "                                                                    body2[non_zero,:num_predicates],dim=1)\n",
    "        #head of rule with the two predicates\n",
    "        preds_r3[non_zero] = preds_r3[non_zero]*F.cosine_similarity(rule_expanded[non_zero,:num_predicates],\n",
    "                                                                    preds_expanded[non_zero,:],dim=1)\n",
    "    \n",
    "    preds_r3 = preds_r3.unsqueeze(1)\n",
    "    preds_r3 = torch.cat((preds_expanded\n",
    "                         ,body1[:,num_predicates:num_predicates+num_constants]\n",
    "                         ,body2[:,num_predicates+num_constants:-2]\n",
    "                         ,preds_r3\n",
    "                         ,body1[:,-1].unsqueeze(1)\n",
    "                         ,body2[:,-1].unsqueeze(1)\n",
    "                         ,-torch.ones(body1.size()[0],1))\n",
    "                        ,dim=1)\n",
    "    \n",
    "    #removing repeated facts and leaving ones with highest score\n",
    "    preds_r3 = leaveTopK(preds_r3,K)\n",
    "\n",
    "    #taking care of third atom\n",
    "    no_preds_left = preds_r3.size()[0]\n",
    "    body3 = facts_tmp.repeat((1,no_preds_left)).view(-1,num_feats_per_fact+2)\n",
    "\n",
    "    preds_r4 = preds_r3.repeat((num_facts,1))\n",
    "\n",
    "    rule_expanded = rules[1].repeat((preds_r4.size()[0],1))\n",
    "    \n",
    "    #unifying second shared constant\n",
    "    p = preds_r4[:,-4] * F.cosine_similarity(preds_r4[:,num_predicates+num_constants:num_predicates+2*num_constants],\n",
    "                                            body3[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "#     nz = p.nonzero().squeeze()\n",
    "    \n",
    "#     p = p * F.cosine_similarity(preds_r4[:,num_predicates+num_constants:num_predicates+2*num_constants],\n",
    "#                                             body3[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    #unifying third body predicate\n",
    "    p = p * F.cosine_similarity(rule_expanded[:,3*num_predicates:],\n",
    "                                            body3[:,:num_predicates],dim=1)\n",
    "    \n",
    "    preds_r5 = torch.cat((preds_r4[:,:num_predicates+num_constants]\n",
    "                         ,body3[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "                         ,p.unsqueeze(1)\n",
    "                         ,preds_r4[:,num_predicates+2*num_constants+1:num_predicates+2*num_constants+3]\n",
    "                         ,body3[:,-1].unsqueeze(1))\n",
    "                        ,dim=1)\n",
    "#     interpret_conseqs(preds_r5[nz,:],facts_tmp)\n",
    "#     for el in body3[nz,:]:\n",
    "#         print_fact(el)\n",
    "#         break\n",
    "#     preds_r5 = torch.cat((preds_r4[:,:num_predicates+num_constants]\n",
    "#                          ,body3[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "#                          ,p.unsqueeze(1)\n",
    "#                          ,preds_r4[:,[-3,-2]]\n",
    "#                          ,body3[:,-1].unsqueeze(1))\n",
    "#                         ,dim=1)\n",
    "    #removing repeated facts and leaving ones with highest score\n",
    "    preds_r5 = leaveTopK(preds_r5,K)\n",
    "#     out = torch.cat((preds_r1,preds_r2,preds_r3),dim=0)\n",
    "    out = torch.cat((preds_r1,preds_r5),dim=0)\n",
    "#     out = torch.cat((preds_r1,preds_r3),dim=0)\n",
    "#     print(\"fws took %s\" % (time.time() - start_time))\n",
    "    return out\n",
    "\n",
    "#Find maximum similarity for each consequence in the set of facts contained in target\n",
    "#if testing is true it finds the consequence with maximum similarity for each target\n",
    "#otherwise finds the target with maximum similarity for each consequence\n",
    "#Inputs: \n",
    "#consequences: result of unrolling the rules for the specified steps with the input facts\n",
    "#target: set of facts that are assumed to be true\n",
    "#testing: returns the probabilities of matched facts, a prediction is considered true if p>0.5\n",
    "def find_max_similarities(consequences,target,testing=False,masking=False):\n",
    "    start_time = time.time()    \n",
    "    num_consequences = consequences.size()[0]\n",
    "    num_targets = target.size()[0]\n",
    "    #add id to the facts\n",
    "    tmp_t = torch.cat((target,torch.range(0,target.size()[0]-1,1).unsqueeze(1))\n",
    "                     ,dim=1)\n",
    "    \n",
    "    #each consequence repeated by the number of targets\n",
    "    if testing:\n",
    "        #for each target find max similarity across consequences\n",
    "        tmp_c = consequences.repeat(num_targets,1)\n",
    "        tmp_t = tmp_t.repeat(1,num_consequences).view(-1,num_feats_per_fact+1)\n",
    "    else:\n",
    "        #for each consequence compute the similarity with all targets\n",
    "        tmp_c = consequences.repeat(1,num_targets).view(-1,num_feats_per_fact+4)\n",
    "        tmp_t = target.repeat(num_consequences,1)\n",
    "\n",
    "    #first constant\n",
    "    sim = F.cosine_similarity(tmp_c[:,num_predicates:num_predicates+num_constants],\n",
    "                              tmp_t[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    if masking:\n",
    "        #let's mask input facts\n",
    "        #repeat the indices over columns\n",
    "        mask = tmp_t[:,-1].unsqueeze(1)\n",
    "        mask = mask.repeat(1,3)\n",
    "        #substract from the fact indices\n",
    "        mask = tmp_c[:,num_predicates+2*num_constants+1:] - mask\n",
    "        #get the rows which have zeros\n",
    "        mask = (mask==0).nonzero()\n",
    "        #only the first column (containing the rows to be 0ed)\n",
    "        mask = mask[:,0].unique()\n",
    "        sim[mask] = 0\n",
    "    \n",
    "    sim = sim * F.cosine_similarity(tmp_c[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "                                   ,tmp_t[:,num_predicates+num_constants:num_predicates+2*num_constants]\n",
    "                                   ,dim=1)\n",
    "    sim = sim * F.cosine_similarity(tmp_c[:,:num_predicates], \n",
    "                                    tmp_t[:,:num_predicates],dim=1)\n",
    "\n",
    "#     sim = sim + tmp_c[:,-4]*lamb2\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #for each consequence/target, get the maximum simlarity with the set of targets/consequences\n",
    "    if testing:\n",
    "        sim = sim.view(-1,num_consequences)\n",
    "    else:\n",
    "        sim = sim.view(-1,num_targets)\n",
    "    m, idx = torch.max(sim,dim=1)\n",
    "#     print(\"fms took %s\" % (time.time() - start_time))\n",
    "    if testing:\n",
    "        return m, consequences[idx,:]\n",
    "    return m,target[idx,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.0474,  0.9078]), tensor([ 0.0936,  0.9041,  0.0457,  0.9111,  0.0514,  0.9294,  0.0938,\n",
      "         0.9041])]\n",
      "auc 0.625\n",
      "[tensor([ 0.0521,  0.9135]), tensor([ 0.9138,  0.4042,  0.0846,  0.9100,  0.2230,  0.9246,  0.9150,\n",
      "         0.4112])]\n",
      "auc 0.7614583333333333\n",
      "[tensor([ 0.0651,  0.9062]), tensor([ 0.4428,  0.7500,  0.0576,  0.9181,  0.1035,  0.8967,  0.5031,\n",
      "         0.8991])]\n",
      "auc 0.125\n",
      "[tensor([ 0.0440,  0.9107]), tensor([ 0.9106,  0.0652,  0.8249,  0.4352,  0.8393,  0.4114,  0.9117,\n",
      "         0.0718])]\n",
      "auc 0.9635416666666667\n",
      "[tensor([ 0.0637,  0.9045]), tensor([ 0.0980,  0.9042,  0.0505,  0.9103,  0.0768,  0.9057,  0.0976,\n",
      "         0.9048])]\n",
      "auc 0.125\n",
      "[tensor([ 0.0555,  0.9117]), tensor([ 0.9136,  0.3453,  0.0804,  0.9066,  0.1299,  0.9016,  0.9114,\n",
      "         0.2984])]\n",
      "auc 0.7614583333333333\n",
      "[tensor([ 0.0400,  0.9397]), tensor([ 0.7251,  0.4466,  0.1098,  0.9011,  0.0985,  0.9082,  0.9056,\n",
      "         0.4944])]\n",
      "auc 0.7614583333333333\n",
      "[tensor([ 0.0513,  0.9065]), tensor([ 0.2559,  0.9308,  0.0442,  0.9145,  0.0522,  0.9203,  0.2412,\n",
      "         0.9308])]\n",
      "auc 0.625\n",
      "[tensor([ 0.0341,  0.9168]), tensor([ 0.6400,  0.3683,  0.0721,  0.9072,  0.0771,  0.8096,  0.8598,\n",
      "         0.5158])]\n",
      "auc 0.8117559523809523\n",
      "[tensor([ 0.0302,  0.9575]), tensor([ 0.8719,  0.1378,  0.0885,  0.9042,  0.2165,  0.8756,  0.8831,\n",
      "         0.1339])]\n",
      "auc 0.7490451388888889\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "####TRAINING\n",
    "#dbg -> cherrypicked\n",
    "# core_rel = Variable(knowledge_pos[[0,1,3]])\n",
    "# target = Variable(knowledge_pos[2]).unsqueeze(0)\n",
    "# target = Variable(knowledge_pos[[2,4],:])\n",
    "#####sampling\n",
    "target = Variable(knowledge_pos)\n",
    "no_samples = 40\n",
    "\n",
    "num_iters = 50\n",
    "learning_rate = .1\n",
    "lamb = 1\n",
    "lamb2 = 0\n",
    "\n",
    "steps = 1\n",
    "num_rules = 3\n",
    "epsilon=.001\n",
    "\n",
    "K = 400 ##For top K\n",
    "\n",
    "#hyperparameter search\n",
    "# lambdas = [1,2,5,0.3,0.8]\n",
    "with open('s3_auc-pr_9-1_clip','w') as f:\n",
    "    # for lamb in lambdas:\n",
    "    suc_rate_neigh = 0\n",
    "    suc_rate_locin = 0\n",
    "    accuracies = []\n",
    "    for _ in range(10):\n",
    "        K_tmp = K\n",
    "        #rules should be:\n",
    "        #r1(x,y) <- r2(y,x)\n",
    "        #r1(x,y) <- r2(x,z),r3(z,x)\n",
    "        rules = [Variable(torch.rand(1*num_predicates), requires_grad=True),\n",
    "                 # Variable(torch.rand(2*num_predicates), requires_grad=True),\n",
    "                 Variable(torch.rand(4*num_predicates), requires_grad=True)]\n",
    "        # rules = [Variable(torch.rand(num_predicates), requires_grad=True),\n",
    "        #          Variable(torch.Tensor([1, 1]), requires_grad=True)]\n",
    "        f.write('random_rules' + str(rules) + '\\n')\n",
    "        optimizer = torch.optim.Adam([\n",
    "                {'params': rules}], \n",
    "                lr = learning_rate)\n",
    "\n",
    "        criterion = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "        rules_tmp = [torch.zeros_like(rule) for rule in rules]\n",
    "        for epoch in range(num_iters):\n",
    "            for par in optimizer.param_groups:\n",
    "#                 par['params'][2].data.clamp_(min=0.3,max=0.7)\n",
    "                par['params'][1].data.clamp_(min=0.1,max=0.9)\n",
    "                par['params'][0].data.clamp_(min=0.1,max=0.9)\n",
    "\n",
    "            # core_rel = Variable(knowledge_pos[core_rel])\n",
    "            core_rel = sample_neighbors(no_samples,data)\n",
    "            # target = Variable(knowledge_pos)\n",
    "            optimizer.zero_grad()\n",
    "            facts = torch.cat((core_rel, Variable(torch.ones(core_rel.size()[0], 1))), 1)\n",
    "            #will accumulate predictions separately to compare with target facts\n",
    "            consequences = forward_step(facts,K_tmp)\n",
    "            for step in range(1,steps):\n",
    "                tmp = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "                tmp = forward_step(tmp,K_tmp)\n",
    "                consequences = torch.cat((consequences,tmp),dim=0)\n",
    "            #LOSS\n",
    "            loss = 0\n",
    "            m, matches = find_max_similarities(consequences,core_rel,testing=True,masking=True)\n",
    "            loss = torch.sum(m*(1 - matches[:,-4]))\n",
    "#             print(epoch, 'losssssssssssssssssssss',loss.data[0])\n",
    "            # print(sum([torch.sum(rules_tmp[i]-rules[i]) for i in range(num_rules)]))\n",
    "#             if loss < 10**-6 or sum([torch.sum(torch.abs(rules_tmp[i]-rules[i])) for i in range(num_rules)])<10**-5:\n",
    "#                 break\n",
    "            rules_tmp = [r.clone() for r in rules]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(rules)\n",
    "        ###### printing and saving AUC\n",
    "        K_tmp = 2000\n",
    "        facts = torch.cat((knowledge_pos, Variable(torch.ones(knowledge_pos.size()[0], 1))), 1)\n",
    "        consequences = forward_step(facts,K_tmp)\n",
    "        for step in range(1,steps):\n",
    "            tmp = torch.cat((facts,consequences[:,:-3]),dim=0)\n",
    "            tmp = forward_step(tmp,K_tmp)\n",
    "            consequences = torch.cat((consequences,tmp),dim=0)\n",
    "        false_set = get_false_set(test,comp)\n",
    "        test_plus_false = torch.cat((test,false_set))\n",
    "        m,matches = find_max_similarities(consequences,test_plus_false,testing=True)\n",
    "        p = matches[:,-4]\n",
    "        true_vals = np.ones(test.size()[0])\n",
    "        true_vals = np.concatenate((true_vals,np.zeros(false_set.size()[0])))\n",
    "        prec,rec,_ = prc(true_vals,(m*p).detach().numpy()) #assume that no unification is a score of 0\n",
    "        auc_tmp = auc(rec,prec)\n",
    "        print('auc',auc_tmp)\n",
    "        f.write('rules' + str(rules) + '\\n')\n",
    "        f.write('auc' + str(auc_tmp) + '\\n')\n",
    "#######Writting results\n",
    "    #     suc_neigh, suc_locIn = False,False\n",
    "    #     if F.cosine_similarity(rules[0],torch.Tensor([0,1,0,1]),dim=0)>0.5:\n",
    "    #         suc_neigh = True\n",
    "    #     if F.cosine_similarity(rules[1],torch.Tensor([1,0,1,0,1,0]),dim=0)>0.5:\n",
    "    #         suc_locIn = True\n",
    "    #     if suc_neigh:\n",
    "    #         suc_rate_neigh+=1\n",
    "    #     if suc_locIn:\n",
    "    #         suc_rate_locin+=1\n",
    "    #     f.write('lamb '+str(lamb)+'\\n')\n",
    "    #     f.write('train_loss '+str(loss)+'\\n')\n",
    "    #     f.write('rules '+str(rules)+'\\n')\n",
    "    #     f.write('suc_neigh '+str(suc_neigh)+'\\n')\n",
    "    #     f.write('suc_locIn '+str(suc_locIn)+'\\n')\n",
    "    #     #computing test results\n",
    "    #     print('computing test results')\n",
    "    #     K_tmp = 250\n",
    "    #     facts = torch.cat((knowledge_pos, Variable(torch.ones(knowledge_pos.size()[0], 1))), 1)\n",
    "    #     consequences = forward_step(facts,K_tmp)\n",
    "    #     for step in range(1,steps):\n",
    "    #         tmp = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "    #         tmp = forward_step(tmp,K_tmp)\n",
    "    #         consequences = torch.cat((consequences,tmp),dim=0)\n",
    "    #     m,matches = find_max_similarities(consequences,test,testing=True)\n",
    "    #     p = matches[:,-3]\n",
    "    #     true_positives = m[p>0.5]\n",
    "    #     true_positives = (true_positives>0.5).nonzero()\n",
    "    #     true_positives = true_positives.size()[0]\n",
    "    #     ts_accuracy = true_positives/test.size()[0]\n",
    "    #     accuracies.append(ts_accuracy)\n",
    "    #     f.write('ts_accuracy '+str(ts_accuracy)+'\\n')\n",
    "    #     f.flush()\n",
    "    # f.write('#############RESULTS###############'+'\\n')\n",
    "    # f.write('lamb '+str(lamb)+'\\n')\n",
    "    # f.write('suc_rate_neigh '+str(suc_rate_neigh)+'\\n')\n",
    "    # f.write('suc_rate_locin '+str(suc_rate_locin)+'\\n')\n",
    "    # accuracies = np.array(accuracies)\n",
    "    # f.write('mean accuracy ' + str(np.mean(accuracies)) +'\\n')\n",
    "    # f.write('std accuracy ' + str(np.std(accuracies))+'\\n')\n",
    "    # f.write('####################################'+'\\n')\n",
    "    # f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "torch.Size([3143, 548])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
      "         12,  13,  14,  15,  16,  17,  18,  19,  21,  22,  23,  41,\n",
      "         69,  71])\n",
      "0 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn bulgaria europe\n",
      "implying facts\n",
      "tensor([ 419.,  234.,   84.])\n",
      "neighborOf bulgaria romania\n",
      "locatedIn romania eastern_europe\n",
      "locatedIn eastern_europe europe\n",
      "---------------\n",
      "1 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn burkina_faso africa\n",
      "implying facts\n",
      "tensor([ 427.,  289.,  322.])\n",
      "neighborOf burkina_faso togo\n",
      "locatedIn togo western_africa\n",
      "locatedIn western_africa africa\n",
      "---------------\n",
      "2 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn djibouti africa\n",
      "implying facts\n",
      "tensor([ 491.,  266.,   82.])\n",
      "neighborOf djibouti somalia\n",
      "locatedIn somalia eastern_africa\n",
      "locatedIn eastern_africa africa\n",
      "---------------\n",
      "3 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn ecuador americas\n",
      "implying facts\n",
      "tensor([ 503.,  224.,  269.])\n",
      "neighborOf ecuador peru\n",
      "locatedIn peru south_america\n",
      "locatedIn south_america americas\n",
      "---------------\n",
      "4 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn egypt africa\n",
      "implying facts\n",
      "tensor([ 505.,  154.,  209.])\n",
      "neighborOf egypt libya\n",
      "locatedIn libya northern_africa\n",
      "locatedIn northern_africa africa\n",
      "---------------\n",
      "5 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn eritrea africa\n",
      "implying facts\n",
      "tensor([ 512.,   90.,   82.])\n",
      "neighborOf eritrea ethiopia\n",
      "locatedIn ethiopia eastern_africa\n",
      "locatedIn eastern_africa africa\n",
      "---------------\n",
      "6 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn french_guiana americas\n",
      "implying facts\n",
      "tensor([ 534.,  277.,  269.])\n",
      "neighborOf french_guiana suriname\n",
      "locatedIn suriname south_america\n",
      "locatedIn south_america americas\n",
      "---------------\n",
      "7 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn germany europe\n",
      "implying facts\n",
      "tensor([ 543.,   19.,  324.])\n",
      "neighborOf germany austria\n",
      "locatedIn austria western_europe\n",
      "locatedIn western_europe europe\n",
      "---------------\n",
      "8 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn ghana africa\n",
      "implying facts\n",
      "tensor([ 554.,  289.,  322.])\n",
      "neighborOf ghana togo\n",
      "locatedIn togo western_africa\n",
      "locatedIn western_africa africa\n",
      "---------------\n",
      "9 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn guyana americas\n",
      "implying facts\n",
      "tensor([ 573.,  277.,  269.])\n",
      "neighborOf guyana suriname\n",
      "locatedIn suriname south_america\n",
      "locatedIn south_america americas\n",
      "---------------\n",
      "10 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn indonesia asia\n",
      "implying facts\n",
      "tensor([ 595.,  165.,  267.])\n",
      "neighborOf indonesia malaysia\n",
      "locatedIn malaysia south-eastern_asia\n",
      "locatedIn south-eastern_asia asia\n",
      "---------------\n",
      "11 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn iraq asia\n",
      "implying facts\n",
      "tensor([ 605.,  126.,  275.])\n",
      "neighborOf iraq iran\n",
      "locatedIn iran southern_asia\n",
      "locatedIn southern_asia asia\n",
      "---------------\n",
      "12 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn jordan asia\n",
      "implying facts\n",
      "tensor([ 628.,  131.,  323.])\n",
      "neighborOf jordan israel\n",
      "locatedIn israel western_asia\n",
      "locatedIn western_asia asia\n",
      "---------------\n",
      "13 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn monaco europe\n",
      "implying facts\n",
      "tensor([ 709.,   98.,  324.])\n",
      "neighborOf monaco france\n",
      "locatedIn france western_europe\n",
      "locatedIn western_europe europe\n",
      "---------------\n",
      "14 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn norway europe\n",
      "implying facts\n",
      "tensor([ 756.,  235.,   84.])\n",
      "neighborOf norway russia\n",
      "locatedIn russia eastern_europe\n",
      "locatedIn eastern_europe europe\n",
      "---------------\n",
      "15 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn saudi_arabia asia\n",
      "implying facts\n",
      "tensor([ 822.,  233.,  323.])\n",
      "neighborOf saudi_arabia qatar\n",
      "locatedIn qatar western_asia\n",
      "locatedIn western_asia asia\n",
      "---------------\n",
      "16 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn spain europe\n",
      "implying facts\n",
      "tensor([ 867.,   98.,  324.])\n",
      "neighborOf spain france\n",
      "locatedIn france western_europe\n",
      "locatedIn western_europe europe\n",
      "---------------\n",
      "17 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn sudan africa\n",
      "implying facts\n",
      "tensor([ 877.,  154.,  209.])\n",
      "neighborOf sudan libya\n",
      "locatedIn libya northern_africa\n",
      "locatedIn northern_africa africa\n",
      "---------------\n",
      "18 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn tanzania africa\n",
      "implying facts\n",
      "tensor([ 901.,   81.,  184.])\n",
      "neighborOf tanzania dr_congo\n",
      "locatedIn dr_congo middle_africa\n",
      "locatedIn middle_africa africa\n",
      "---------------\n",
      "19 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn thailand asia\n",
      "implying facts\n",
      "tensor([ 908.,   45.,  267.])\n",
      "neighborOf thailand cambodia\n",
      "locatedIn cambodia south-eastern_asia\n",
      "locatedIn south-eastern_asia asia\n",
      "---------------\n",
      "20 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn united_states americas\n",
      "implying facts\n",
      "tensor([ 945.,   47.,  210.])\n",
      "neighborOf united_states canada\n",
      "locatedIn canada northern_america\n",
      "locatedIn northern_america americas\n",
      "---------------\n",
      "21 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn venezuela americas\n",
      "implying facts\n",
      "tensor([ 955.,   37.,  269.])\n",
      "neighborOf venezuela brazil\n",
      "locatedIn brazil south_america\n",
      "locatedIn south_america americas\n",
      "---------------\n",
      "22 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn zimbabwe africa\n",
      "implying facts\n",
      "tensor([ 977.,  327.,   82.])\n",
      "neighborOf zimbabwe zambia\n",
      "locatedIn zambia eastern_africa\n",
      "locatedIn eastern_africa africa\n",
      "---------------\n",
      "23 consequence\n",
      "tensor(1.00000e-02 *\n",
      "       1.6083)\n",
      "locatedIn malawi south_sudan\n",
      "implying facts\n",
      "tensor([ 688.,  906.,  933.])\n",
      "neighborOf malawi tanzania\n",
      "neighborOf tanzania uganda\n",
      "neighborOf uganda south_sudan\n",
      "---------------\n",
      "24 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn turkey asia\n",
      "implying facts\n",
      "tensor([ 925.,  283.,  323.])\n",
      "neighborOf turkey syria\n",
      "locatedIn syria western_asia\n",
      "locatedIn western_asia asia\n",
      "---------------\n",
      "25 consequence\n",
      "tensor(0.4166)\n",
      "locatedIn north_korea europe\n",
      "implying facts\n",
      "tensor([ 753.,  235.,   84.])\n",
      "neighborOf north_korea russia\n",
      "locatedIn russia eastern_europe\n",
      "locatedIn eastern_europe europe\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "non_zero = m.nonzero().squeeze()\n",
    "print(m)\n",
    "print(consequences.size())\n",
    "print(non_zero)\n",
    "# interpret_conseqs(test_plus_false,facts)\n",
    "interpret_conseqs(matches[non_zero,:],facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc 0.9436378205128205\n"
     ]
    }
   ],
   "source": [
    "# rules = [torch.Tensor([ 0.01,  0.99,  0.01,  0.99]),\n",
    "#          torch.Tensor([ 0.99,  0.01,  0.01,0.99,  0.01,0.99,0.99,  0.01])]\n",
    "rules = [torch.Tensor([ 0.0440,  0.9107]),\n",
    "         torch.Tensor([ 0.9106,  0.0652,  0.8249,  0.4352,  0.8393,  0.4114,  0.9117,\n",
    "         0.0718])]\n",
    "K_tmp = 2000\n",
    "facts = torch.cat((knowledge_pos, Variable(torch.ones(knowledge_pos.size()[0], 1))), 1)\n",
    "consequences = forward_step(facts,K_tmp)\n",
    "# for step in range(1,steps):\n",
    "#     tmp = torch.cat((facts,consequences[:,:-3]),dim=0)\n",
    "#     tmp = forward_step(tmp,K_tmp)\n",
    "#     consequences = torch.cat((consequences,tmp),dim=0)\n",
    "false_set = get_false_set(test,comp)\n",
    "test_plus_false = torch.cat((test,false_set))\n",
    "m,matches = find_max_similarities(consequences,test_plus_false,testing=True)\n",
    "p = matches[:,-4]\n",
    "true_vals = np.ones(test.size()[0])\n",
    "true_vals = np.concatenate((true_vals,np.zeros(false_set.size()[0])))\n",
    "prec,rec,_ = prc(true_vals,(m*p).detach().numpy()) #assume that no unification is a score of 0\n",
    "auc_tmp = auc(rec,prec)\n",
    "print('auc',auc_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.3000,  0.7000,  0.3000,  0.7000]), tensor([ 0.3000,  0.7000,  0.3000,  0.7000,  0.3000,  0.7000,  0.3000,\n",
      "         0.7000])]\n",
      "30\n",
      "224\n",
      "mmmmmmmm tensor([ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,\n",
      "         0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         1.,  0.,  0.,  0.,  1.,  1.])\n",
      "ground target\n",
      "neighborOf denmark germany\n",
      "matched fact\n",
      "neighborOf denmark germany\n",
      "implying facts\n",
      "neighborOf germany denmark\n",
      "---------------\n",
      "ground target\n",
      "neighborOf liechtenstein switzerland\n",
      "matched fact\n",
      "neighborOf liechtenstein switzerland\n",
      "implying facts\n",
      "neighborOf switzerland liechtenstein\n",
      "---------------\n",
      "ground target\n",
      "neighborOf france luxembourg\n",
      "matched fact\n",
      "neighborOf france luxembourg\n",
      "implying facts\n",
      "neighborOf luxembourg france\n",
      "---------------\n",
      "ground target\n",
      "locatedIn switzerland western_europe\n",
      "matched fact\n",
      "locatedIn switzerland western_europe\n",
      "implying facts\n",
      "neighborOf switzerland austria\n",
      "neighborOf austria liechtenstein\n",
      "locatedIn liechtenstein western_europe\n",
      "---------------\n",
      "ground target\n",
      "neighborOf switzerland liechtenstein\n",
      "matched fact\n",
      "neighborOf switzerland liechtenstein\n",
      "implying facts\n",
      "neighborOf liechtenstein switzerland\n",
      "---------------\n",
      "ground target\n",
      "neighborOf luxembourg france\n",
      "matched fact\n",
      "neighborOf luxembourg france\n",
      "implying facts\n",
      "neighborOf france luxembourg\n",
      "---------------\n",
      "ground target\n",
      "neighborOf austria hungary\n",
      "matched fact\n",
      "neighborOf austria hungary\n",
      "implying facts\n",
      "neighborOf hungary austria\n",
      "---------------\n",
      "ground target\n",
      "neighborOf hungary austria\n",
      "matched fact\n",
      "neighborOf hungary austria\n",
      "implying facts\n",
      "neighborOf austria hungary\n",
      "---------------\n",
      "ground target\n",
      "neighborOf france switzerland\n",
      "matched fact\n",
      "neighborOf france switzerland\n",
      "implying facts\n",
      "neighborOf france luxembourg\n",
      "neighborOf luxembourg germany\n",
      "neighborOf germany switzerland\n",
      "---------------\n",
      "ground target\n",
      "neighborOf germany denmark\n",
      "matched fact\n",
      "neighborOf germany denmark\n",
      "implying facts\n",
      "neighborOf denmark germany\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "rules = [torch.Tensor([ 0.3,  0.7,  0.3,  0.7]),\n",
    "         torch.Tensor([ 0.3,  0.7,0.3,  0.7,0.3,  0.7,0.3,  0.7])]\n",
    "\n",
    "lamb2=0\n",
    "print(rules)\n",
    "facts = sample_neighbors(30,data)\n",
    "target = facts.clone()\n",
    "facts = torch.cat((facts, Variable(torch.ones(facts.size()[0], 1))), 1)\n",
    "facts_tmp = facts.clone()\n",
    "consequences = forward_step(facts_tmp,1000)\n",
    "# print('consequences')\n",
    "# interpret_conseqs(consequences,facts)\n",
    "# for step in range(1,steps):\n",
    "#     tmp = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "#     tmp = forward_step(tmp,150)\n",
    "#     consequences = torch.cat((consequences,tmp),dim=0)\n",
    "m,matches = find_max_similarities(consequences,target,testing=True,masking=True)\n",
    "print('mmmmmmmm',m)\n",
    "non_zero = m.nonzero().squeeze()\n",
    "mtp = matches[non_zero,:]\n",
    "# print(mtp[:,[0,1]])\n",
    "interpret_matches(matches,facts,m)\n",
    "# print(matches)\n",
    "# print(m*(1-matches[:,-3]))\n",
    "# #writting an interpreter\n",
    "# allFacts = torch.cat((facts,consequences[:,:-2]),dim=0)\n",
    "# print(target[torch.round(1-m).nonzero(),:num_predicates])\n",
    "# print(matches[:,[-2,-1]])\n",
    "# print(m)\n",
    "# for i in range(target.size()[0]):\n",
    "#     print_fact(target[i,:])\n",
    "# for i in range(target.size()[0]):\n",
    "#     print('-----------------')\n",
    "#     print_fact(target[i,:])\n",
    "#     print_fact(matches[i,:])\n",
    "#     print_fact(facts[matches[i,-2].type(torch.LongTensor),:])\n",
    "#     if matches[i,-1] != -1:\n",
    "#         print_fact(facts[matches[i,-1].type(torch.LongTensor),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def interpret_matches(matches,facts,m):\n",
    "    for i in range(matches.size()[0]):\n",
    "        if m[i]==0:\n",
    "            continue\n",
    "        el = matches[i,:]\n",
    "        fact = facts[i,:]\n",
    "        print('ground target')\n",
    "        print_fact(fact)\n",
    "        print('matched fact')\n",
    "        print_fact(el)\n",
    "        print('implying facts')\n",
    "        for f in el[[-3,-2,-1]]:\n",
    "            if int(f) != -1:\n",
    "                print_fact(facts[int(f),:])\n",
    "            else:\n",
    "                break\n",
    "        print('---------------')\n",
    "def interpret_conseqs(consequences,facts):\n",
    "    for i in range(consequences.size()[0]):\n",
    "        el = consequences[i,:]\n",
    "        print(i,'consequence')\n",
    "        if el[-4]==0:\n",
    "            print('zero probability')\n",
    "        print(el[-4])\n",
    "        print_fact(el)\n",
    "        print('implying facts')\n",
    "        print(el[[-3,-2,-1]])\n",
    "        for f in el[[-3,-2,-1]]:\n",
    "            if int(f) != -1:\n",
    "#                 print(facts[int(f),:])\n",
    "                print_fact(facts[int(f),:])\n",
    "            else:\n",
    "                break\n",
    "        print('---------------')\n",
    "# interpret_conseqs(consequences,facts)\n",
    "\n",
    "# interpret_matches(matches,facts,m)\n",
    "# interpret_conseqs(consequences,facts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
