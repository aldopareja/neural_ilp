{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4373,  0.3305],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(0.8555)\n",
      "tensor(0.8544) tensor(0.3305)\n",
      "tensor(0.4614) tensor(0.)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8452) tensor(0.)\n",
      "tensor(0.4614) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.1742,  0.3582,  0.5841,  0.8313])]\n",
      "0 losssssssssssssssssssss tensor(0.7176)\n",
      "tensor([[ 0.7280,  0.4666],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(0.6603)\n",
      "tensor(0.8972) tensor(0.4666)\n",
      "tensor(0.5027) tensor(0.)\n",
      "tensor(0.1082) tensor(0.)\n",
      "tensor(0.5027) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.2742,  0.2582,  0.6841,  0.7313])]\n",
      "1 losssssssssssssssssssss tensor(0.5813)\n",
      "tensor([[ 0.9182,  0.6027],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(0.4466)\n",
      "tensor(0.9312) tensor(0.6027)\n",
      "tensor(0.5389) tensor(0.)\n",
      "tensor(0.1466) tensor(0.)\n",
      "tensor(0.5389) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.3736,  0.1612,  0.7827,  0.6355])]\n",
      "2 losssssssssssssssssssss tensor(0.4388)\n",
      "tensor([[ 0.9910,  0.7278],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(0.2787)\n",
      "tensor(0.9550) tensor(0.7278)\n",
      "tensor(0.5674) tensor(0.)\n",
      "tensor(0.1799) tensor(0.)\n",
      "tensor(0.5674) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.4641,  0.0627,  0.8821,  0.5394])]\n",
      "3 losssssssssssssssssssss tensor(0.3050)\n",
      "tensor([[ 1.0000,  0.8315],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(0.1685)\n",
      "tensor(0.9687) tensor(0.8315)\n",
      "tensor(0.5873) tensor(0.)\n",
      "tensor(0.2059) tensor(0.)\n",
      "tensor(0.5873) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.5399,  0.0000,  0.9812,  0.4418])]\n",
      "4 losssssssssssssssssssss tensor(0.1945)\n",
      "tensor([[ 1.0000,  0.8947],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(0.1053)\n",
      "tensor(0.9777) tensor(0.8947)\n",
      "tensor(0.6022) tensor(0.)\n",
      "tensor(0.2268) tensor(0.)\n",
      "tensor(0.6022) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.6041,  0.0000,  1.0000,  0.3431])]\n",
      "5 losssssssssssssssssssss tensor(0.1253)\n",
      "tensor([[ 1.0000,  0.9436],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(1.00000e-02 *\n",
      "       5.6402)\n",
      "tensor(0.9840) tensor(0.9436)\n",
      "tensor(0.6140) tensor(0.)\n",
      "tensor(0.2440) tensor(0.)\n",
      "tensor(0.6140) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.6593,  0.0000,  1.0000,  0.2445])]\n",
      "6 losssssssssssssssssssss tensor(1.00000e-02 *\n",
      "       7.1517)\n",
      "tensor([[ 1.0000,  0.9788],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(1.00000e-02 *\n",
      "       2.1232)\n",
      "tensor(0.9885) tensor(0.9788)\n",
      "tensor(0.6234) tensor(0.)\n",
      "tensor(0.2583) tensor(0.)\n",
      "tensor(0.6234) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.7076,  0.0000,  1.0000,  0.1473])]\n",
      "7 losssssssssssssssssssss tensor(1.00000e-02 *\n",
      "       3.2452)\n",
      "tensor([[ 1.0000,  0.9971],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "loss1 tensor(1.00000e-03 *\n",
      "       2.8893)\n",
      "tensor(0.9918) tensor(0.9971)\n",
      "tensor(0.6312) tensor(0.)\n",
      "tensor(0.2705) tensor(0.)\n",
      "tensor(0.6312) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.7501,  0.0000,  1.0000,  0.0538])]\n",
      "8 losssssssssssssssssssss tensor(1.00000e-02 *\n",
      "       1.1020)\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n",
      "loss1 tensor(0.)\n",
      "tensor(0.9943) tensor(1.)\n",
      "tensor(0.6376) tensor(0.)\n",
      "tensor(0.2810) tensor(0.)\n",
      "tensor(0.6376) tensor(0.)\n",
      "[tensor([ 0.4445,  0.0133]), tensor([ 0.7879,  0.0000,  1.0000,  0.0000])]\n",
      "9 losssssssssssssssssssss tensor(1.00000e-03 *\n",
      "       5.7383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aldo.pareja/miniconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:181: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/aldo.pareja/miniconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:185: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from pudb import set_trace;\n",
    "\n",
    "with open('data/countries_s1_dbg_locin') as f:\n",
    "    facts = f.read().splitlines()\n",
    "facts = [el.split(',') for el in facts]\n",
    "preds = [fact[0] for fact in facts]\n",
    "subjs = [fact[1] for fact in facts]\n",
    "objs = [fact[2] for fact in facts]\n",
    "\n",
    "unique = sorted(list(set(preds)))\n",
    "num_unique = len(unique)\n",
    "num_predicates = num_unique\n",
    "predsToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToPreds = dict(zip(range(num_unique),unique))\n",
    "\n",
    "unique = sorted(list(set(subjs+objs)))\n",
    "num_unique = len(unique)\n",
    "num_constants = num_unique\n",
    "consToIdx = dict(zip(unique,range(num_unique)))\n",
    "idxToCons = dict(zip(range(num_unique),unique))\n",
    "\n",
    "facts = np.array([(predsToIdx[preds[i]],consToIdx[subjs[i]], consToIdx[objs[i]]) for i in range(len(facts))])\n",
    "data = np.zeros((num_predicates,num_constants,num_constants))\n",
    "\n",
    "#data: idx0->predicate, idx1->subj, idx2->obj\n",
    "data[facts[:,0],facts[:,1],facts[:,2]] = 1\n",
    "no_facts = int(np.sum(data))\n",
    "data = torch.from_numpy(data)\n",
    "\n",
    "predicates = torch.eye(num_predicates)\n",
    "constants = torch.eye(num_constants)\n",
    "\n",
    "knowledge_pos = (data==1).nonzero()\n",
    "data_aux = knowledge_pos\n",
    "knowledge_pos = torch.cat((predicates[knowledge_pos[:,0]],\n",
    "                           constants[knowledge_pos[:,1]],\n",
    "                           constants[knowledge_pos[:,2]]),dim=1)\n",
    "num_facts = knowledge_pos.size()[0]\n",
    "num_feats_per_fact = knowledge_pos.size()[1]\n",
    "\n",
    "#helps removing repeated predicted facts -> same embeddings and constants, probably different scores\n",
    "#this is of complexity K^3, could be optimized\n",
    "def leaveTopK(preds,K):\n",
    "    _,idx = torch.sort(preds[:,-1],descending=True)\n",
    "    preds = preds[idx,:]\n",
    "    out = preds[0,:].unsqueeze(0)\n",
    "    for i in range(1,K):\n",
    "        t = preds[i,:].unsqueeze(0)\n",
    "        m,_ = torch.max(F.cosine_similarity(t[:,:-1].repeat(out.size()[0],1),out[:,:-1]),dim=0)\n",
    "        if m<1:\n",
    "            out = torch.cat((out,t),dim=0)\n",
    "    return out\n",
    "\n",
    "####FORWARD CHAINING\n",
    "#input: some facts that can either be ground or predicted in previous steps\n",
    "#output: predicted facts in this specific step (outer loop must gather all of them)\n",
    "#computes the predictions of applying each rule to the facts, giving a score to each of them.\n",
    "#leaves only topK scoring fact for each applied rule (not for the whole thing)\n",
    "def forward_step(facts):\n",
    "    num_facts = facts.size()[0]\n",
    "    #rule 1\n",
    "    # b1(x,y)<-b1(y,x)\n",
    "    # rule_expanded = rules[0].expand(facts[:,:num_predicates].size())\n",
    "    # preds_r1 = F.cosine_similarity(rule_expanded,facts[:,:num_predicates],dim=1)\n",
    "    # preds_r1 = preds_r1*facts[:,-1]\n",
    "    # preds_r1 = preds_r1.unsqueeze(1)\n",
    "    # preds_r1 = torch.cat((rule_expanded,\n",
    "    #                      facts[:,num_predicates+num_constants:-1],\n",
    "    #                      facts[:,num_predicates:num_predicates+num_constants],\n",
    "    #                      preds_r1),dim=1)\n",
    "    # preds_r1 = leaveTopK(preds_r1,K)\n",
    "    #rule 2\n",
    "    # b1(x,y)<-b2(x,z),b2(z,y)\n",
    "    body1 = facts.repeat((1,num_facts)).view(-1,num_feats_per_fact+1)\n",
    "    body2 = facts.repeat((num_facts,1))\n",
    "    rule_expanded = rules[1].repeat(body1.size()[0],1)\n",
    "    #previous scores\n",
    "    preds_r2 = body1[:,-1]*body2[:,-1]\n",
    "    #predicate of body1 with predicate of rule\n",
    "    preds_r2 = preds_r2*F.cosine_similarity(rule_expanded[:,num_predicates:],body1[:,:num_predicates],dim=1)\n",
    "    #predicate of body2 with predicate of rule\n",
    "    preds_r2 = preds_r2*F.cosine_similarity(rule_expanded[:,num_predicates:],body2[:,:num_predicates],dim=1)\n",
    "    #similarity between shared constants\n",
    "    preds_r2 = preds_r2*F.cosine_similarity(body1[:,num_predicates+num_constants:-1],\n",
    "                                            body2[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    preds_r2 = preds_r2.unsqueeze(1)\n",
    "    preds_r2 = torch.cat((rule_expanded[:,:num_predicates]\n",
    "                         ,body1[:,num_predicates:num_predicates+num_constants]\n",
    "                         ,body2[:,num_predicates+num_constants:-1]\n",
    "                         ,preds_r2)\n",
    "                        ,dim=1)\n",
    "    #removing repeated facts and leaving ones with highest score\n",
    "    preds_r2 = leaveTopK(preds_r2,K)\n",
    "    # out = torch.cat((preds_r1,preds_r2),dim=0)\n",
    "    # return out\n",
    "    return preds_r2\n",
    "\n",
    "    \n",
    "####TRAINING\n",
    "#dbg -> cherrypicked\n",
    "core_rel = Variable(knowledge_pos[[0,1]])\n",
    "target = Variable(knowledge_pos[2]).unsqueeze(0)\n",
    "\n",
    "#####sampling\n",
    "# target = Variable(knowledge_pos)\n",
    "# no_samples = 100\n",
    "\n",
    "num_iters = 200\n",
    "learning_rate = .1\n",
    "lamb = 10\n",
    "\n",
    "steps = 1\n",
    "num_rules = 2\n",
    "epsilon=.001\n",
    "\n",
    "K = 4 ##For top K\n",
    "\n",
    "#rules should be:\n",
    "#r1(x,y) <- r1(y,x)\n",
    "#r1(x,y) <- r2(x,z),r2(z,x)\n",
    "rules = [Variable(torch.rand(num_predicates), requires_grad=True),\n",
    "         Variable(torch.rand(2*num_predicates), requires_grad=True)]\n",
    "# rules = [Variable(torch.rand(num_predicates), requires_grad=True),\n",
    "#          Variable(torch.Tensor([1, 1]), requires_grad=True)]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "        {'params': rules}], \n",
    "        lr = learning_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "for epoch in range(num_iters):\n",
    "    for par in optimizer.param_groups:\n",
    "        par['params'][1].data.clamp_(min=0.,max=1.)\n",
    "    # # ##sampling\n",
    "    # core_rel = torch.randperm(no_facts)\n",
    "    # # target = core_rel[no_samples:]\n",
    "    # core_rel = core_rel[:no_samples]\n",
    "\n",
    "    # core_rel = Variable(knowledge_pos[core_rel])\n",
    "    # target = Variable(knowledge_pos)\n",
    "    optimizer.zero_grad()\n",
    "    facts = torch.cat((core_rel, Variable(torch.ones(core_rel.size()[0], 1))), 1)\n",
    "    #will accumulate predictions separately to compare with target facts\n",
    "    consequences = forward_step(facts)\n",
    "    for step in range(1,steps):\n",
    "        tmp = torch.cat((consequences,facts),dim=0)\n",
    "        tmp = forward_step(tmp)\n",
    "        consequences = torch.cat((consequences,tmp),dim=0)\n",
    "    #LOSS\n",
    "    loss = 0\n",
    "    num_consequences = consequences.size()[0]\n",
    "    num_targets = target.size()[0]\n",
    "    #each consequence repeated by the number of targets\n",
    "    tmp_c = consequences.repeat(1,target.size()[0]).view(-1,num_feats_per_fact+1)\n",
    "    #all targets repeated number of consequences\n",
    "    tmp_t = target.repeat(num_consequences,1)\n",
    "    #for each consequence compute the similarity with all targets\n",
    "    sim = F.cosine_similarity(tmp_c[:,:num_predicates],target[:,:num_predicates],dim=1)\n",
    "    sim = sim * F.cosine_similarity(tmp_c[:,num_predicates:num_predicates+num_constants],\n",
    "                                    target[:,num_predicates:num_predicates+num_constants],dim=1)\n",
    "    sim = sim * F.cosine_similarity(tmp_c[:,num_predicates+num_constants:-1],\n",
    "                                    target[:,num_predicates+num_constants:],dim=1)\n",
    "    # sim = F.cosine_similarity(consequences[:,:-1],target)\n",
    "    #for each consequence, get the maximum simlarity with the set of targets\n",
    "    sim = sim.view(-1,num_targets)\n",
    "    m, _ = torch.max(sim,dim=1)\n",
    "    print(torch.stack((m,consequences[:,-1]),dim=1))\n",
    "    #the loss is min(lamb*p,1-p*m)\n",
    "    loss = torch.sum(torch.min(lamb*consequences[:,-1],1- consequences[:,-1]*m))\n",
    "    print('loss1',loss)\n",
    "    loss2 = 0\n",
    "    for cons in consequences:\n",
    "        m, indi = torch.max(F.cosine_similarity(cons[:-1].view(1,-1).expand(target.size()),target),0)\n",
    "        indi=indi.data[0]\n",
    "        loss2 += torch.min(torch.stack((lamb*cons[-1],1-cons[-1]*m)))\n",
    "        print(m,cons[-1])\n",
    "    print(rules)\n",
    "    print(epoch, 'losssssssssssssssssssss',loss2.data[0])\n",
    "    if loss < 10**-6:\n",
    "        break\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.],\n",
      "        [ 0.,  1.]])\n",
      "tensor([[ 0.1000,  1.0000],\n",
      "        [ 0.1000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "out = torch.Tensor([[0,1],[0,1]])\n",
    "t = torch.Tensor([0.1,1]).unsqueeze(0)\n",
    "print(o)\n",
    "print(t.repeat(out.size()[0],1))\n",
    "F.cosine_similarity(t.repeat(out.size()[0],1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
